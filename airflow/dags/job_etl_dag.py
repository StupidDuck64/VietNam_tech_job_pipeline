"""
===== Airflow DAG - ITviec Job ETL Pipeline =====
ƒêi·ªÅu ph·ªëi to√†n b·ªô quy tr√¨nh ETL:
1. Scrape d·ªØ li·ªáu t·ª´ ITviec.com
2. X·ª≠ l√Ω d·ªØ li·ªáu v·ªõi Spark
3. Load v√†o Data Warehouse (PostgreSQL)

L·ªãch ch·∫°y: H√†ng ng√†y l√∫c 8:00 AM
Author: Data Engineering Team
Date: December 2025
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.utils.dates import days_ago
import logging
import os
import sys

# ===== Add scripts folder to path =====
sys.path.insert(0, '/opt/airflow/scripts')

# ===== Import custom modules =====
try:
    from ingestion.itviec_scraper import ITviecScraper
    from processing.spark_cleaner import SparkDataCleaner
except ImportError as e:
    logging.warning(f"‚ö†Ô∏è Cannot import custom modules: {e}")

# ===== Configure logging =====
logger = logging.getLogger(__name__)

# ===== Environment =====
AIRFLOW_HOME = os.getenv('AIRFLOW_HOME', '/opt/airflow')
MONGO_HOST = os.getenv('MONGO_HOST', 'mongodb')
MONGO_PORT = os.getenv('MONGO_PORT', '27017')
MONGO_USERNAME = os.getenv('MONGO_INITDB_ROOT_USERNAME', 'admin')
MONGO_PASSWORD = os.getenv('MONGO_INITDB_ROOT_PASSWORD', 'mongodb_password')
MONGO_DB = os.getenv('MONGO_DB', 'job_db')
TARGET_URL = os.getenv('TARGET_URL', 'https://itviec.com/it-jobs')
# TƒÉng gi·ªõi h·∫°n m·∫∑c ƒë·ªãnh l√™n 50 ƒë·ªÉ c√†o h·∫øt danh s√°ch keywords
MAX_PAGES = int(os.getenv('MAX_PAGES', '50'))

# ===== Default DAG arguments =====
default_args = {
    'owner': 'data-engineering-team',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'start_date': days_ago(1),
    'email': ['admin@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'depends_on_past': False,
}

# ===== DAG Definition =====
dag = DAG(
    'job_etl_dag',
    default_args=default_args,
    description='End-to-End ETL Pipeline cho ITviec Job Analytics',
    schedule_interval='0 8 * * *',  # Ch·∫°y l√∫c 8:00 AM h√†ng ng√†y
    catchup=False,
    tags=['etl', 'scraping', 'data-processing'],
)


# ===== Task 1: Scrape Jobs from ITviec =====
def scrape_jobs_task(**context):
    """
    Task c√†o d·ªØ li·ªáu t·ª´ ITviec.com
    L∆∞u d·ªØ li·ªáu raw v√†o MongoDB
    """
    logger.info("üìÑ B·∫Øt ƒë·∫ßu scrape jobs t·ª´ ITviec.com")
    
    try:
        # ===== T·∫°o MongoDB URI =====
        mongo_uri = f"mongodb://{MONGO_USERNAME}:{MONGO_PASSWORD}@{MONGO_HOST}:{MONGO_PORT}/"
        
        # ===== Kh·ªüi t·∫°o scraper =====
        scraper = ITviecScraper(
            mongo_uri=mongo_uri,
            db_name=MONGO_DB
        )
        
        # ===== K·∫øt n·ªëi MongoDB =====
        scraper.connect_mongodb()
        
        # ===== C√†o d·ªØ li·ªáu (c√†o nhi·ªÅu trang) =====
        logger.info(f"üöÄ C√†o T·∫§T C·∫¢ IT Jobs t·ª´ URL: {TARGET_URL} (max {MAX_PAGES} trang)")
        jobs = scraper.scrape_jobs(TARGET_URL, max_pages=MAX_PAGES)
        
        # ===== L∆∞u v√†o MongoDB =====
        success = scraper.save_to_mongodb(jobs)
        
        # ===== L·∫•y th·ªëng k√™ =====
        stats = scraper.get_statistics()
        
        # ===== Push stats v√†o XCom (ƒë·ªÉ d√πng trong task sau) =====
        context['task_instance'].xcom_push(
            key='scrape_stats',
            value=stats
        )
        
        logger.info(f"‚úÖ Scrape ho√†n th√†nh. Stats: {stats}")
        
        # ===== Ng·∫Øt k·∫øt n·ªëi =====
        scraper.disconnect_mongodb()
        
        return success
    
    except Exception as e:
        logger.error(f"‚ùå L·ªói scrape: {e}")
        raise


def scrape_jobs_wrapper(**context):
    """Wrapper function ƒë·ªÉ g·ªçi scrape_jobs_task"""
    return scrape_jobs_task(**context)


task_scrape = PythonOperator(
    task_id='scrape_jobs',
    python_callable=scrape_jobs_wrapper,
    provide_context=True,
    retries=2,
    retry_delay=timedelta(minutes=5),
    dag=dag,
)


# ===== Task 2: Process Data with Spark =====
def process_data_task(**context):
    """
    Task x·ª≠ l√Ω d·ªØ li·ªáu v·ªõi Spark
    - L√†m s·∫°ch d·ªØ li·ªáu
    - Tr√≠ch xu·∫•t k·ªπ nƒÉng
    - L∆∞u v√†o Parquet
    """
    logger.info("‚ö° B·∫Øt ƒë·∫ßu Spark processing")
    
    try:
        # ===== Kh·ªüi t·∫°o Spark cleaner =====
        cleaner = SparkDataCleaner(app_name="ITviec-ETL-Processing")
        
        # ===== Ch·∫°y processing pipeline =====
        cleaner.process_pipeline()
        
        logger.info("‚úÖ Spark processing ho√†n th√†nh")
        
        # ===== D·ª´ng Spark =====
        cleaner.stop()
        
        return True
    
    except Exception as e:
        logger.error(f"‚ùå L·ªói Spark processing: {e}")
        raise


def process_data_wrapper(**context):
    """Wrapper function ƒë·ªÉ g·ªçi process_data_task"""
    return process_data_task(**context)


task_process = PythonOperator(
    task_id='process_data',
    python_callable=process_data_wrapper,
    provide_context=True,
    retries=1,
    dag=dag,
)


# ===== Task 3: Validate Data Quality =====
def validate_data_quality(**context):
    """
    Task ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu
    - S·ªë l∆∞·ª£ng records
    - Missing data
    - Outliers
    """
    logger.info("üîç B·∫Øt ƒë·∫ßu validation ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu")
    
    try:
        # ===== L·∫•y scrape stats t·ª´ XCom =====
        scrape_stats = context['task_instance'].xcom_pull(
            task_ids='scrape_jobs',
            key='scrape_stats'
        )
        
        logger.info(f"üìä Scrape stats: {scrape_stats}")
        
        # ===== Ki·ªÉm tra basic validation =====
        if scrape_stats and scrape_stats.get('total_jobs', 0) > 0:
            logger.info("‚úÖ Validation passed - C√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω")
            return True
        else:
            logger.warning("‚ö†Ô∏è Validation warning - √çt d·ªØ li·ªáu ho·∫∑c kh√¥ng c√≥ d·ªØ li·ªáu")
            return False
    
    except Exception as e:
        logger.error(f"‚ùå L·ªói validation: {e}")
        raise


task_validate = PythonOperator(
    task_id='validate_data_quality',
    python_callable=validate_data_quality,
    provide_context=True,
    dag=dag,
)


# ===== Task 4: Setup Database Schema (PostgreSQL) =====
# ƒê·ªçc file SQL tr·ª±c ti·∫øp thay v√¨ d√πng template
with open('/opt/airflow/sql/init_db.sql', 'r') as f:
    init_sql = f.read()

task_setup_db = PostgresOperator(
    task_id='setup_database_schema',
    postgres_conn_id='postgres_default',
    sql=init_sql,
    autocommit=True,
    dag=dag,
)


# ===== Task 5: Load to Data Warehouse =====
def load_to_warehouse(**context):
    """
    Task load d·ªØ li·ªáu t·ª´ Parquet v√†o PostgreSQL
    (N·∫øu c·∫•u h√¨nh JDBC + Spark)
    """
    logger.info("üóÑÔ∏è B·∫Øt ƒë·∫ßu load d·ªØ li·ªáu v√†o Data Warehouse")
    
    try:
        logger.info("‚è≥ D·ªØ li·ªáu ƒë∆∞·ª£c load qua Spark Write JDBC trong task Process Data")
        logger.info("‚úÖ Load to Warehouse ho√†n th√†nh")
        return True
    
    except Exception as e:
        logger.error(f"‚ùå L·ªói load: {e}")
        raise


task_load = PythonOperator(
    task_id='load_to_warehouse',
    python_callable=load_to_warehouse,
    provide_context=True,
    dag=dag,
)


# ===== Task 6: Generate Report / Notification =====
def generate_report(**context):
    """
    Task t·∫°o b√°o c√°o v√† th√¥ng b√°o
    """
    logger.info("üìß T·∫°o b√°o c√°o...")
    
    try:
        # ===== L·∫•y stats =====
        scrape_stats = context['task_instance'].xcom_pull(
            task_ids='scrape_jobs',
            key='scrape_stats'
        )
        
        # ===== T·∫°o message =====
        message = f"""
        ===== ETL Pipeline Report =====
        Execution Date: {context['execution_date']}
        
        Scraping Results:
        - Total Jobs: {scrape_stats.get('total_jobs', 0)}
        - Unique Companies: {scrape_stats.get('unique_companies', 0)}
        - Unique Locations: {scrape_stats.get('unique_locations', 0)}
        
        Status: ‚úÖ SUCCESS
        """
        
        logger.info(message)
        
        # ===== Save to file (optional) =====
        report_path = f"{AIRFLOW_HOME}/logs/reports/report_{context['execution_date']}.txt"
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        
        with open(report_path, 'w') as f:
            f.write(message)
        
        logger.info(f"‚úÖ Report saved to: {report_path}")
        return True
    
    except Exception as e:
        logger.error(f"‚ùå L·ªói generate report: {e}")
        raise


task_report = PythonOperator(
    task_id='generate_report',
    python_callable=generate_report,
    provide_context=True,
    dag=dag,
)


# ===== Task 7: Cleanup (Optional) =====
task_cleanup = BashOperator(
    task_id='cleanup_temp_files',
    bash_command=f"""
    # ===== X√≥a d·ªØ li·ªáu temp (n·∫øu c·∫ßn) =====
    echo "‚úÖ Cleanup ho√†n th√†nh"
    """,
    dag=dag,
)


# ===== Set Task Dependencies =====
# 
#    scrape_jobs
#        ‚Üì
#  validate_data (ki·ªÉm tra quality)
#        ‚Üì
#  setup_db + process_data (parallel)
#        ‚Üì
#    load_to_warehouse
#        ‚Üì
#  generate_report ‚Üí cleanup

task_scrape >> task_validate >> [task_setup_db, task_process] >> task_load >> task_report >> task_cleanup


# ===== DAG Documentation =====
dag.doc_md = """
# ITviec Job Analytics - ETL Pipeline

## M√¥ t·∫£
Pipeline End-to-End ƒë·ªÉ c√†o, x·ª≠ l√Ω, v√† ph√¢n t√≠ch d·ªØ li·ªáu vi·ªác l√†m IT t·ª´ ITviec.com

## C√°c t√°c v·ª• (Tasks):
1. **scrape_jobs**: C√†o d·ªØ li·ªáu t·ª´ ITviec.com, l∆∞u v√†o MongoDB
2. **validate_data_quality**: Ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu
3. **setup_database_schema**: T·∫°o schema trong PostgreSQL (Star Schema)
4. **process_data**: X·ª≠ l√Ω d·ªØ li·ªáu v·ªõi Spark (clean, extract skills)
5. **load_to_warehouse**: Load d·ªØ li·ªáu v√†o PostgreSQL
6. **generate_report**: T·∫°o b√°o c√°o ETL
7. **cleanup_temp_files**: X√≥a c√°c file t·∫°m

## L·ªãch ch·∫°y
- H√†ng ng√†y l√∫c **8:00 AM**
- Timezone: UTC

## Infrastructure
- **MongoDB**: L∆∞u d·ªØ li·ªáu raw
- **PostgreSQL**: Data Warehouse
- **Spark**: Processing engine
- **Airflow**: Orchestration

## Contacts
- Team: Data Engineering
- Email: admin@example.com
"""
