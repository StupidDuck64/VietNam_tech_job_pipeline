# VN IT Job Analytics - End-to-End Data Engineering Project

Dá»± Ã¡n Data Engineering "End-to-End" Ä‘á»ƒ cÃ o vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u viá»‡c lÃ m IT tá»« ITviec.com.

## ğŸ“‹ Tá»•ng Quan

Luá»“ng dá»¯ liá»‡u (ETL Pipeline):
1. **Ingestion**: CÃ o dá»¯ liá»‡u tá»« ITviec.com â†’ LÆ°u vÃ o MongoDB (JSON)
2. **Processing**: Äá»c tá»« MongoDB â†’ LÃ m sáº¡ch & trÃ­ch xuáº¥t skill vá»›i Spark â†’ LÆ°u Parquet
3. **Serving**: Ghi dá»¯ liá»‡u vÃ o PostgreSQL Data Warehouse
4. **Orchestration**: Airflow Ä‘iá»u phá»‘i quy trÃ¬nh hÃ ng ngÃ y

## ğŸ—‚ï¸ Cáº¥u TrÃºc Project

```
vn-it-job-analytics/
â”œâ”€â”€ airflow/                   # Cáº¥u hÃ¬nh Airflow
â”‚   â”œâ”€â”€ dags/                  # DAG definitions
â”‚   â”‚   â””â”€â”€ job_etl_dag.py
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ plugins/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Dá»¯ liá»‡u JSON thÃ´
â”‚   â””â”€â”€ processed/             # Dá»¯ liá»‡u Parquet Ä‘Ã£ xá»­ lÃ½
â”œâ”€â”€ docker/                    # Docker configs
â”‚   â”œâ”€â”€ airflow.Dockerfile
â”‚   â””â”€â”€ spark.Dockerfile
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ itviec_scraper.py
â”‚   â””â”€â”€ processing/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ spark_cleaner.py
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ init_db.sql
â”‚   â””â”€â”€ queries.sql
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸš€ HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng

### 1. Chuáº©n Bá»‹ MÃ´i TrÆ°á»ng
```bash
# Clone project
git clone <repo-url>
cd vn-it-job-analytics

# CÃ i Ä‘áº·t Python packages
pip install -r requirements.txt
```

### 2. Khá»Ÿi Äá»™ng Infrastructure vá»›i Docker
```bash
docker-compose up -d
```

CÃ¡c service sáº½ cháº¡y:
- **PostgreSQL**: `localhost:5432`
- **MongoDB**: `localhost:27017`
- **Airflow Webserver**: `http://localhost:8080`
- **Spark Master**: `http://localhost:8888` (náº¿u cáº¥u hÃ¬nh)

### 3. Cháº¡y Pipeline Thá»§ CÃ´ng (Test)
```bash
# Cháº¡y scraper
python scripts/ingestion/itviec_scraper.py

# Cháº¡y Spark processing
spark-submit scripts/processing/spark_cleaner.py
```

### 4. Theo DÃµi Airflow
- Má»Ÿ browser: `http://localhost:8080`
- Username/Password: `airflow/airflow` (default)
- Trigger DAG `job_etl_dag` tá»« UI

## ğŸ› ï¸ CÃ´ng Nghá»‡ Sá»­ Dá»¥ng

- **Scraping**: Python, Requests, BeautifulSoup4
- **Database**: PostgreSQL (Data Warehouse), MongoDB (Raw Data)
- **Processing**: PySpark
- **Orchestration**: Apache Airflow
- **Containerization**: Docker, Docker Compose

## ğŸ“Š TÃ­nh NÄƒng ChÃ­nh

âœ… Tá»± Ä‘á»™ng cÃ o dá»¯ liá»‡u viá»‡c lÃ m tá»« ITviec.com
âœ… TrÃ­ch xuáº¥t ká»¹ nÄƒng (Skills) tá»« Job Description
âœ… LÃ m sáº¡ch vÃ  chuáº©n hÃ³a dá»¯ liá»‡u (lÆ°Æ¡ng, vá»‹ trÃ­, etc)
âœ… LÆ°u trá»¯ dá»¯ liá»‡u theo Star Schema (Fact + Dimension)
âœ… Tá»± Ä‘á»™ng hÃ³a vá»›i Airflow scheduler (cháº¡y hÃ ng ngÃ y 8 AM)
âœ… Há»— trá»£ phÃ¢n tÃ­ch vÃ  bÃ¡o cÃ¡o

## ğŸ“ Notes

- **Cáº§n thay Ä‘á»•i**: Cáº­p nháº­t `TARGET_URL` trong `.env` náº¿u muá»‘n cÃ o tá»« website khÃ¡c
- **Data Privacy**: HÃ£y kiá»ƒm tra `robots.txt` vÃ  terms of service trÆ°á»›c khi scrape
- **Performance**: Náº¿u RAM háº¡n cháº¿, chá»‰nh láº¡i `spark.executor.memory` trong config

## ğŸ”— Tham Kháº£o

- [Airflow Documentation](https://airflow.apache.org/)
- [PySpark Guide](https://spark.apache.org/docs/latest/api/python/)
- [MongoDB PyMongo](https://pymongo.readthedocs.io/)

---

**Created**: December 2025
**Status**: In Development âœ¨
