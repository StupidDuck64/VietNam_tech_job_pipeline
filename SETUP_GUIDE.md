# üöÄ VN IT Job Analytics - Setup & Run Guide

**H∆∞·ªõng d·∫´n chi ti·∫øt t·ª´ng b∆∞·ªõc ƒë·ªÉ setup v√† ch·∫°y d·ª± √°n Data Engineering End-to-End**

---

## üìã Y√™u c·∫ßu H·ªá Th·ªëng

- **OS**: Windows, macOS, Linux
- **Docker Desktop**: v4.0+ (https://www.docker.com/products/docker-desktop)
- **Python**: 3.9+ (n·∫øu ch·∫°y locally)
- **RAM t·ªëi thi·ªÉu**: 8GB (recommend 16GB cho Spark + Airflow)
- **Disk**: √çt nh·∫•t 10GB

---

## üîß B∆∞·ªõc 1: Chu·∫©n B·ªã M√¥i Tr∆∞·ªùng

### 1.1 C√†i ƒë·∫∑t Docker Desktop

1. **Windows/Mac**: 
   - Download t·ª´ https://www.docker.com/products/docker-desktop
   - C√†i ƒë·∫∑t nh∆∞ b√¨nh th∆∞·ªùng
   - Kh·ªüi ƒë·ªông Docker Desktop

2. **Linux** (Ubuntu/Debian):
   ```bash
   sudo apt-get install docker.io docker-compose
   sudo usermod -aG docker $USER
   ```

### 1.2 Verify Docker Installation

```bash
docker --version
docker-compose --version
```

N·∫øu hi·ªÉn th·ªã version ‚Üí ‚úÖ Docker ƒë√£ s·∫µn s√†ng

---

## üìÇ B∆∞·ªõc 2: Clone / Setup Project

### 2.1 Di chuy·ªÉn v√†o project folder

```bash
cd c:\Users\c9283\PyCharmMiscProject\DE_project\DE_Project_01\vn-it-job-analytics
```

### 2.2 Verify folder structure

```bash
# Windows PowerShell
dir

# Linux/Mac
ls -la
```

B·∫°n s·∫Ω th·∫•y:
```
‚îú‚îÄ‚îÄ airflow/
‚îú‚îÄ‚îÄ data/
‚îú‚îÄ‚îÄ docker/
‚îú‚îÄ‚îÄ scripts/
‚îú‚îÄ‚îÄ sql/
‚îú‚îÄ‚îÄ docker-compose.yaml
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ SETUP_GUIDE.md (file n√†y)
```

### 2.3 Ki·ªÉm tra .env file

File `.env` ph·∫£i ch·ª©a c√°c bi·∫øn m√¥i tr∆∞·ªùng. N·∫øu kh√¥ng c√≥, t·∫°o file m·ªõi:

```bash
# Xem n·ªôi dung (Linux/Mac)
cat .env

# Xem n·ªôi dung (Windows PowerShell)
Get-Content .env
```

---

## üê≥ B∆∞·ªõc 3: Kh·ªüi ƒê·ªông Infrastructure

### 3.1 Build Docker Images

```bash
# Di chuy·ªÉn v√†o project folder (n·∫øu ch∆∞a)
cd vn-it-job-analytics

# Build t·∫•t c·∫£ images
docker-compose build
```

‚è≥ **Qu√° tr√¨nh n√†y s·∫Ω m·∫•t 5-10 ph√∫t** (t√πy v√†o internet speed)

### 3.2 Kh·ªüi ƒë·ªông t·∫•t c·∫£ Services

```bash
# Kh·ªüi ƒë·ªông t·∫•t c·∫£ container ·ªü background
docker-compose up -d
```

‚úÖ L·ªánh s·∫Ω:
- Kh·ªüi ƒë·ªông PostgreSQL
- Kh·ªüi ƒë·ªông MongoDB  
- Kh·ªüi ƒë·ªông Airflow Webserver & Scheduler
- Kh·ªüi ƒë·ªông Spark Master & Worker

### 3.3 Ki·ªÉm tra Status

```bash
# Li·ªát k√™ t·∫•t c·∫£ running containers
docker-compose ps
```

B·∫°n s·∫Ω th·∫•y output nh∆∞:

```
NAME                    STATUS
postgres_db             Up 2 minutes
mongodb_service         Up 2 minutes
airflow_webserver       Up 1 minute
airflow_scheduler       Up 1 minute
spark_master            Up 1 minute
spark_worker            Up 1 minute
```

### 3.4 Ki·ªÉm tra Logs (Debug)

N·∫øu c√≥ container b·ªã "Exited", ki·ªÉm tra logs:

```bash
# Xem logs c·ªßa Airflow Webserver
docker-compose logs airflow-webserver

# Xem logs c·ªßa MongoDB
docker-compose logs mongodb

# Follow logs (stream realtime)
docker-compose logs -f airflow-webserver
```

---

## üåê B∆∞·ªõc 4: Truy C·∫≠p C√°c Service

Sau khi t·∫•t c·∫£ containers ƒë√£ up, b·∫°n c√≥ th·ªÉ truy c·∫≠p:

| Service | URL | Username | Password |
|---------|-----|----------|----------|
| **Airflow** | `http://localhost:8080` | `airflow` | `airflow` |
| **PostgreSQL** | `localhost:5432` | `airflow_user` | `airflow_password` |
| **MongoDB** | `localhost:27017` | `admin` | `mongodb_password` |
| **Spark Master** | `http://localhost:8888` | - | - |

### 4.1 Ki·ªÉm tra Airflow Webserver

1. M·ªü browser
2. Truy c·∫≠p: `http://localhost:8080`
3. Login v·ªõi: `airflow / airflow`
4. B·∫°n s·∫Ω th·∫•y DAG `job_etl_dag` trong list

---

## üß™ B∆∞·ªõc 5: Ch·∫°y Pipeline

### 5.1 C√°ch 1: Trigger t·ª´ Airflow UI

1. M·ªü `http://localhost:8080`
2. T√¨m DAG `job_etl_dag`
3. Nh·∫•n v√†o DAG name
4. Nh·∫•n n√∫t **Trigger DAG** (m≈©i t√™n xanh)
5. Xem task execution graph

### 5.2 C√°ch 2: Ch·∫°y Script Th·ªß C√¥ng

#### Ch·∫°y Scraper:
```bash
# Ch·∫°y scraper script (l·∫•y d·ªØ li·ªáu t·ª´ ITviec)
docker-compose exec airflow-webserver python /opt/airflow/scripts/ingestion/itviec_scraper.py
```

#### Ch·∫°y Spark Processing:
```bash
# Ch·∫°y spark processing script (l√†m s·∫°ch & tr√≠ch xu·∫•t skill)
docker-compose exec airflow-webserver spark-submit /opt/airflow/scripts/processing/spark_cleaner.py
```

---

## üìä B∆∞·ªõc 6: Xem K·∫øt Qu·∫£

### 6.1 Ki·ªÉm tra d·ªØ li·ªáu trong MongoDB

```bash
# M·ªü MongoDB shell
docker-compose exec mongodb mongosh -u admin -p mongodb_password

# Trong MongoDB shell:
use job_db
db.raw_jobs.find().limit(1).pretty()  # Xem 1 job
db.raw_jobs.count()                    # ƒê·∫øm t·ªïng jobs
```

### 6.2 Ki·ªÉm tra d·ªØ li·ªáu trong PostgreSQL

```bash
# M·ªü PostgreSQL client
docker-compose exec postgres psql -U airflow_user -d airflow_db

# Trong PostgreSQL:
SELECT COUNT(*) FROM fact_jobs;
SELECT * FROM fact_jobs LIMIT 5;
\d fact_jobs  -- Xem schema
```

### 6.3 Xem d·ªØ li·ªáu Parquet

```bash
# D·ªØ li·ªáu Parquet ƒë∆∞·ª£c l∆∞u t·∫°i:
# data/processed/jobs_processed/

# C√≥ th·ªÉ d√πng Pandas ƒë·ªÉ read
python
>>> import pandas as pd
>>> df = pd.read_parquet('data/processed/jobs_processed')
>>> df.head()
```

---

## üìà B∆∞·ªõc 7: Ch·∫°y Queries Ph√¢n T√≠ch

### 7.1 K·∫øt n·ªëi PostgreSQL v√† ch·∫°y queries

```bash
docker-compose exec postgres psql -U airflow_user -d airflow_db -f /opt/airflow/sql/queries.sql
```

### 7.2 Queries m·∫´u:

```sql
-- Top Skills ƒë∆∞·ª£c t√¨m ki·∫øm
SELECT skill_name, COUNT(*) as count 
FROM dim_skills 
GROUP BY skill_name 
ORDER BY count DESC LIMIT 10;

-- Salary theo location
SELECT location, AVG(salary_max) as avg_salary 
FROM fact_jobs 
GROUP BY location 
ORDER BY avg_salary DESC;

-- Companies ƒëang tuy·ªÉn nhi·ªÅu nh·∫•t
SELECT company_name, COUNT(*) as job_count 
FROM fact_jobs 
GROUP BY company_name 
ORDER BY job_count DESC;
```

---

## üõë D·ª´ng & Cleanup

### D·ª´ng t·∫•t c·∫£ containers:
```bash
docker-compose down
```

### X√≥a t·∫•t c·∫£ d·ªØ li·ªáu (volumes):
```bash
docker-compose down -v
```

‚ö†Ô∏è **C·∫£nh b√°o**: L·ªánh tr√™n s·∫Ω x√≥a t·∫•t c·∫£ d·ªØ li·ªáu trong MongoDB & PostgreSQL!

---

## üêõ Troubleshooting

### Problem 1: "Port 5432 already in use"

**Gi·∫£i ph√°p**: 
```bash
# T√¨m process s·ª≠ d·ª•ng port 5432
# Windows:
netstat -ano | findstr :5432

# D·ª´ng container c≈©
docker-compose down
docker ps -a  # xem t·∫•t c·∫£ containers
docker rm <container-id>
```

### Problem 2: "MongoDBConnectionError"

**Gi·∫£i ph√°p**:
```bash
# Xem logs MongoDB
docker-compose logs mongodb

# Restart MongoDB
docker-compose restart mongodb

# Ki·ªÉm tra MongoDB status
docker-compose exec mongodb mongosh -u admin -p mongodb_password --eval "db.adminCommand('ping')"
```

### Problem 3: "Spark executor is not starting"

**Gi·∫£i ph√°p**:
```bash
# RAM kh√¥ng ƒë·ªß - gi·∫£m Spark executor memory
# Ch·ªânh l·∫°i docker-compose.yaml:
# SPARK_EXECUTOR_MEMORY=1g (thay v√¨ 2g)

# Restart Spark
docker-compose restart spark-master spark-worker
```

### Problem 4: "DAG is not showing in Airflow UI"

**Gi·∫£i ph√°p**:
```bash
# Ki·ªÉm tra DAG syntax
python -m py_compile airflow/dags/job_etl_dag.py

# Refresh Airflow UI (Ctrl+F5 ho·∫∑c clear cache)

# Check logs
docker-compose logs airflow-scheduler
```

---

## üìö T√†i Li·ªáu Tham Kh·∫£o

- [Airflow Docs](https://airflow.apache.org/docs/)
- [PySpark Docs](https://spark.apache.org/docs/latest/api/python/)
- [MongoDB PyMongo](https://pymongo.readthedocs.io/)
- [PostgreSQL Docs](https://www.postgresql.org/docs/)
- [Docker Docs](https://docs.docker.com/)

---

## üìù Notes

1. **Modify TARGET_URL**: Mu·ªën c√†o t·ª´ kh√°c URL? S·ª≠a trong `.env`:
   ```
   TARGET_URL=https://itviec.com/it-jobs/junior-data-engineer
   ```

2. **Adjust Schedule**: Mu·ªën ch·∫°y v√†o l√∫c kh√°c? S·ª≠a trong `job_etl_dag.py`:
   ```python
   schedule_interval='0 8 * * *'  # 8:00 AM m·ªói ng√†y
   ```

3. **Increase Scrape Pages**: Mu·ªën c√†o nhi·ªÅu trang? S·ª≠a trong `job_etl_dag.py`:
   ```python
   jobs = scraper.scrape_jobs(TARGET_URL, max_pages=5)  # Thay 5 t·ª´ 3
   ```

4. **Monitor Airflow Logs**:
   ```bash
   docker-compose logs -f airflow-scheduler
   ```

---

## ‚úÖ Ki·ªÉm danh (Checklist)

- [ ] Docker Desktop ƒë√£ c√†i & ch·∫°y
- [ ] ƒê√£ clone project v√†o th∆∞ m·ª•c
- [ ] ƒê√£ ch·∫°y `docker-compose build`
- [ ] ƒê√£ ch·∫°y `docker-compose up -d`
- [ ] Ki·ªÉm tra `docker-compose ps` ‚Üí T·∫•t c·∫£ containers Up
- [ ] Truy c·∫≠p `http://localhost:8080` ‚Üí Airflow UI
- [ ] Trigger `job_etl_dag` th√†nh c√¥ng
- [ ] Ki·ªÉm tra d·ªØ li·ªáu trong MongoDB
- [ ] Ki·ªÉm tra d·ªØ li·ªáu trong PostgreSQL
- [ ] Ch·∫°y sample queries t·ª´ `sql/queries.sql`

---

## üéâ Ho√†n Th√†nh!

N·∫øu t·∫•t c·∫£ b∆∞·ªõc ƒë√£ ho√†n th√†nh, **Ch√∫c m·ª´ng!** üéä

B·∫°n gi·ªù ƒë√£ c√≥ m·ªôt **Data Engineering Pipeline ho√†n ch·ªânh**:
- ‚úÖ T·ª± ƒë·ªông c√†o d·ªØ li·ªáu t·ª´ ITviec.com
- ‚úÖ X·ª≠ l√Ω d·ªØ li·ªáu v·ªõi Spark
- ‚úÖ L∆∞u tr·ªØ trong Data Warehouse (PostgreSQL)
- ‚úÖ ƒêi·ªÅu ph·ªëi v·ªõi Airflow scheduler
- ‚úÖ Ph√¢n t√≠ch & b√°o c√°o d·ªØ li·ªáu

**Next Steps**:
1. S·ª≠a ch·ªânh URL scraping, schedule, parameters theo nhu c·∫ßu
2. Th√™m th√™m logic x·ª≠ l√Ω (skill extraction, validation, etc)
3. T·∫°o th√™m visualizations/dashboards (Tableau, Looker, etc)
4. Deploy l√™n cloud (AWS, GCP, Azure)

**Ch√∫c b·∫°n th√†nh c√¥ng!** üöÄ

---

**Last Updated**: December 2025
