"""
===== Spark Data Cleaner & Skill Extractor =====
Script x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ MongoDB b·∫±ng PySpark
- L√†m s·∫°ch d·ªØ li·ªáu (x√≥a HTML tags, emoji, chu·∫©n h√≥a l∆∞∆°ng)
- Tr√≠ch xu·∫•t k·ªπ nƒÉng (Skills) t·ª´ Job Description
- L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω v√†o Parquet ho·∫∑c PostgreSQL

Author: Data Engineering Team
Date: December 2025
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, when, regexp_replace, lower, trim, split, explode, 
    collect_list, size, regexp_extract_all, lit
)
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType
import logging
import os
from dotenv import load_dotenv
from datetime import datetime

# ===== Load environment variables =====
load_dotenv()

# ===== Config logging =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ===== Environment Variables =====
MONGO_HOST = os.getenv('MONGO_HOST', 'localhost')
MONGO_PORT = os.getenv('MONGO_PORT', '27017')
MONGO_USERNAME = os.getenv('MONGO_INITDB_ROOT_USERNAME', 'admin')
MONGO_PASSWORD = os.getenv('MONGO_INITDB_ROOT_PASSWORD', 'mongodb_password')
MONGO_DB = os.getenv('MONGO_DB', 'job_db')

POSTGRES_HOST = os.getenv('POSTGRES_HOST', 'localhost')
POSTGRES_PORT = os.getenv('POSTGRES_PORT', '5432')
POSTGRES_USER = os.getenv('POSTGRES_USER', 'airflow_user')
POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD', 'airflow_password')
POSTGRES_DB = os.getenv('POSTGRES_DB', 'airflow_db')

# ===== Danh s√°ch k·ªπ nƒÉng IT ph·ªï bi·∫øn =====
SKILLS_LIST = [
    'Python', 'Java', 'JavaScript', 'TypeScript', 'SQL', 'NoSQL', 'MongoDB', 'PostgreSQL',
    'MySQL', 'Spark', 'Hadoop', 'Hive', 'Airflow', 'Docker', 'Kubernetes', 'AWS',
    'Azure', 'GCP', 'Google Cloud', 'Scala', 'Go', 'Rust', 'C\\+\\+', 'C#',
    'React', 'Vue', 'Angular', 'Node.js', 'Django', 'Flask', 'FastAPI', 'Spring',
    'Git', 'Jenkins', 'Elasticsearch', 'Redis', 'Kafka', 'RabbitMQ', 'Tableau',
    'Power BI', 'Looker', 'ETL', 'Linux', 'Unix', 'REST API', 'GraphQL', 'Microservices',
    'Agile', 'Scrum', 'JIRA', 'Pandas', 'NumPy', 'Scikit-learn', 'TensorFlow', 'PyTorch',
    'Machine Learning', 'Deep Learning', 'NLP', 'Computer Vision'
]


class SparkDataCleaner:
    """
    Class x·ª≠ l√Ω d·ªØ li·ªáu v·ªõi Spark
    
    Attributes:
        spark: SparkSession object
    """
    
    def __init__(self, app_name: str = "ITviec-Data-Processor"):
        """
        Kh·ªüi t·∫°o Spark session
        
        Args:
            app_name: T√™n c·ªßa Spark application
        """
        try:
            self.spark = SparkSession.builder \
                .appName(app_name) \
                .config("spark.jars.packages", "org.mongodb.spark:mongo-spark-connector_2.12:10.0.0") \
                .config("spark.mongodb.input.uri", 
                        f"mongodb://{MONGO_USERNAME}:{MONGO_PASSWORD}@{MONGO_HOST}:{MONGO_PORT}/{MONGO_DB}.raw_jobs") \
                .config("spark.mongodb.output.uri",
                        f"mongodb://{MONGO_USERNAME}:{MONGO_PASSWORD}@{MONGO_HOST}:{MONGO_PORT}/{MONGO_DB}.processed_jobs") \
                .getOrCreate()
            
            logger.info("‚úÖ Spark session t·∫°o th√†nh c√¥ng")
        except Exception as e:
            logger.error(f"‚ùå L·ªói t·∫°o Spark session: {e}")
            raise
    
    def read_from_mongodb(self, collection_name: str = "raw_jobs"):
        """
        ƒê·ªçc d·ªØ li·ªáu t·ª´ MongoDB
        
        Args:
            collection_name: T√™n collection trong MongoDB
            
        Returns:
            Spark DataFrame
        """
        try:
            df = self.spark.read.format("mongodb") \
                .option("spark.mongodb.input.uri", 
                        f"mongodb://{MONGO_USERNAME}:{MONGO_PASSWORD}@{MONGO_HOST}:{MONGO_PORT}/{MONGO_DB}.{collection_name}") \
                .load()
            
            logger.info(f"‚úÖ ƒê·ªçc {df.count()} records t·ª´ MongoDB collection: {collection_name}")
            return df
        
        except Exception as e:
            logger.error(f"‚ùå L·ªói ƒë·ªçc t·ª´ MongoDB: {e}")
            raise
    
    def clean_text(self, df, column_name: str) -> object:
        """
        L√†m s·∫°ch text: x√≥a HTML tags, emoji, whitespace th·ª´a
        
        Args:
            df: Spark DataFrame
            column_name: T√™n c·ªôt c·∫ßn l√†m s·∫°ch
            
        Returns:
            DataFrame v·ªõi c·ªôt ƒë√£ l√†m s·∫°ch
        """
        try:
            df = df.withColumn(
                column_name,
                # ===== X√≥a HTML tags =====
                regexp_replace(col(column_name), r'<[^>]+>', '') \
                # ===== X√≥a emoji =====
                .withColumn('cleaned', regexp_replace(col(column_name), 
                    r'[\U0001F300-\U0001F9FF]|\u200d|\ufe0f', '')) \
                # ===== X√≥a whitespace th·ª´a =====
                .withColumn('cleaned', regexp_replace(col('cleaned'), r'\s+', ' ')) \
                # ===== Trim =====
                .withColumn('cleaned', trim(col('cleaned'))) \
                # ===== Lowercase =====
                .withColumn('cleaned', lower(col('cleaned')))
            )
            
            df = df.drop(column_name).withColumnRenamed('cleaned', column_name)
            logger.info(f"‚úÖ L√†m s·∫°ch c·ªôt: {column_name}")
            return df
        
        except Exception as e:
            logger.error(f"‚ùå L·ªói l√†m s·∫°ch text: {e}")
            return df
    
    def normalize_salary(self, df) -> object:
        """
        Chu·∫©n h√≥a c·ªôt l∆∞∆°ng
        - Parse l∆∞∆°ng t·ª´ string (e.g., "$1000 - $2000") th√†nh min/max
        
        Args:
            df: Spark DataFrame
            
        Returns:
            DataFrame v·ªõi c·ªôt salary_min, salary_max
        """
        try:
            # ===== Kh·ªüi t·∫°o c·ªôt m·ªõi =====
            df = df.withColumn('salary_min', lit(None).cast(IntegerType())) \
                   .withColumn('salary_max', lit(None).cast(IntegerType()))
            
            # ===== Parse l∆∞∆°ng t·ª´ string =====
            # V√≠ d·ª•: "$1000 - $2000" -> 1000, 2000
            df = df.withColumn(
                'salary_min',
                when(col('salary').contains('$'),
                     regexp_extract(col('salary'), r'\$(\d+)', 1).cast(IntegerType())
                ).otherwise(None)
            )
            
            df = df.withColumn(
                'salary_max',
                when(col('salary').contains('-'),
                     regexp_extract(col('salary'), r'\$\d+\s*-\s*\$(\d+)', 1).cast(IntegerType())
                ).otherwise(col('salary_min'))
            )
            
            # ===== X√≥a c·ªôt salary c≈© =====
            df = df.drop('salary')
            
            logger.info("‚úÖ Chu·∫©n h√≥a c·ªôt l∆∞∆°ng")
            return df
        
        except Exception as e:
            logger.error(f"‚ùå L·ªói chu·∫©n h√≥a l∆∞∆°ng: {e}")
            return df
    
    def extract_skills(self, df, description_column: str = "description_preview") -> object:
        """
        Tr√≠ch xu·∫•t k·ªπ nƒÉng t·ª´ Job Description
        
        Args:
            df: Spark DataFrame
            description_column: T√™n c·ªôt ch·ª©a job description
            
        Returns:
            DataFrame v·ªõi c·ªôt 'skills' ch·ª©a list c√°c skills
        """
        try:
            # ===== T·∫°o regex pattern t·ª´ danh s√°ch skills =====
            # Join skills v·ªõi | (OR operator)
            skills_pattern = '|'.join(SKILLS_LIST)
            
            # ===== Tr√≠ch xu·∫•t skills t·ª´ description =====
            # S·ª≠ d·ª•ng regex ƒë·ªÉ t√¨m t·∫•t c·∫£ matching skills
            df = df.withColumn(
                'skills',
                explode(
                    when(
                        col(description_column).isNotNull(),
                        regexp_extract_all(
                            lower(col(description_column)),
                            f'(?i)({skills_pattern})'
                        )
                    ).otherwise([])
                )
            )
            
            logger.info("‚úÖ Tr√≠ch xu·∫•t k·ªπ nƒÉng t·ª´ Job Description")
            return df
        
        except Exception as e:
            logger.error(f"‚ùå L·ªói tr√≠ch xu·∫•t skills: {e}")
            return df
    
    def deduplicate_skills(self, df) -> object:
        """
        Lo·∫°i b·ªè skills tr√πng l·∫∑p v√† th√™m skill count
        
        Args:
            df: Spark DataFrame
            
        Returns:
            DataFrame v·ªõi skills ƒë√£ dedup
        """
        try:
            # ===== Group by job v√† collect unique skills =====
            df_agg = df.groupBy('job_id', 'job_title', 'company_name').agg(
                collect_list('skills').alias('skills_list')
            )
            
            # ===== Lo·∫°i b·ªè duplicate trong list =====
            df_agg = df_agg.withColumn(
                'skills',
                when(
                    size(col('skills_list')) > 0,
                    col('skills_list')
                ).otherwise([])
            )
            
            df_agg = df_agg.drop('skills_list')
            
            logger.info("‚úÖ Lo·∫°i b·ªè skills tr√πng l·∫∑p")
            return df_agg
        
        except Exception as e:
            logger.error(f"‚ùå L·ªói dedup skills: {e}")
            return df
    
    def add_metadata(self, df) -> object:
        """
        Th√™m metadata nh∆∞ processed_at, row_id, etc
        
        Args:
            df: Spark DataFrame
            
        Returns:
            DataFrame v·ªõi metadata
        """
        try:
            df = df.withColumn('processed_at', lit(datetime.now().isoformat())) \
                   .withColumn('data_quality_score', lit(1.0))  # Placeholder
            
            logger.info("‚úÖ Th√™m metadata v√†o DataFrame")
            return df
        
        except Exception as e:
            logger.error(f"‚ùå L·ªói th√™m metadata: {e}")
            return df
    
    def write_to_parquet(self, df, output_path: str):
        """
        L∆∞u DataFrame v√†o Parquet format
        
        Args:
            df: Spark DataFrame
            output_path: ƒê∆∞·ªùng d·∫´n output folder
        """
        try:
            df.write.mode("overwrite").parquet(output_path)
            logger.info(f"‚úÖ L∆∞u d·ªØ li·ªáu v√†o Parquet: {output_path}")
        except Exception as e:
            logger.error(f"‚ùå L·ªói l∆∞u Parquet: {e}")
    
    def write_to_postgresql(self, df, table_name: str):
        """
        L∆∞u DataFrame v√†o PostgreSQL
        
        Args:
            df: Spark DataFrame
            table_name: T√™n table trong PostgreSQL
        """
        try:
            jdbc_url = f"jdbc:postgresql://{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"
            
            df.write \
                .format("jdbc") \
                .option("url", jdbc_url) \
                .option("dbtable", table_name) \
                .option("user", POSTGRES_USER) \
                .option("password", POSTGRES_PASSWORD) \
                .option("driver", "org.postgresql.Driver") \
                .mode("append") \
                .save()
            
            logger.info(f"‚úÖ L∆∞u d·ªØ li·ªáu v√†o PostgreSQL table: {table_name}")
        
        except Exception as e:
            logger.error(f"‚ùå L·ªói l∆∞u PostgreSQL: {e}")
    
    def process_pipeline(self):
        """
        Ch·∫°y to√†n b·ªô processing pipeline
        """
        try:
            logger.info("üöÄ B·∫Øt ƒë·∫ßu Spark processing pipeline")
            
            # ===== Step 1: ƒê·ªçc d·ªØ li·ªáu t·ª´ MongoDB =====
            df = self.read_from_mongodb("raw_jobs")
            
            # ===== Step 2: L√†m s·∫°ch text =====
            df = self.clean_text(df, "job_title")
            df = self.clean_text(df, "description_preview")
            df = self.clean_text(df, "company_name")
            
            # ===== Step 3: Chu·∫©n h√≥a l∆∞∆°ng =====
            df = self.normalize_salary(df)
            
            # ===== Step 4: Tr√≠ch xu·∫•t skills =====
            df = self.extract_skills(df, "description_preview")
            
            # ===== Step 5: Th√™m metadata =====
            df = self.add_metadata(df)
            
            # ===== Step 6: Hi·ªÉn th·ªã sample data =====
            logger.info("üìä Sample processed data:")
            df.show(5, truncate=False)
            
            # ===== Step 7: L∆∞u d·ªØ li·ªáu =====
            output_parquet = "data/processed/jobs_processed"
            self.write_to_parquet(df, output_parquet)
            
            # ===== Optional: L∆∞u v√†o PostgreSQL =====
            # self.write_to_postgresql(df, "processed_jobs")
            
            logger.info("‚ú® Spark processing pipeline ho√†n th√†nh!")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói trong pipeline: {e}")
            raise
    
    def stop(self):
        """D·ª´ng Spark session"""
        try:
            self.spark.stop()
            logger.info("‚úÖ Spark session ƒë√£ d·ª´ng")
        except Exception as e:
            logger.error(f"‚ùå L·ªói d·ª´ng Spark session: {e}")


def main():
    """Main function"""
    cleaner = SparkDataCleaner()
    
    try:
        # ===== Ch·∫°y pipeline =====
        cleaner.process_pipeline()
    
    except Exception as e:
        logger.error(f"‚ùå L·ªói chung: {e}")
    
    finally:
        # ===== D·ª´ng Spark =====
        cleaner.stop()


if __name__ == '__main__':
    main()
