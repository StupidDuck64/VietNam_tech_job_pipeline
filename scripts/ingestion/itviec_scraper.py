"""
===== ITviec Job Scraper v·ªõi Selenium =====
Script c√†o d·ªØ li·ªáu vi·ªác l√†m t·ª´ ITviec.com
S·ª≠ d·ª•ng: Selenium + Chrome headless + BeautifulSoup
L∆∞u v√†o: MongoDB

Author: Data Engineering Team  
Date: December 2025
"""

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
from selenium_stealth import stealth
from bs4 import BeautifulSoup
from pymongo import MongoClient, errors
import json
import logging
import os
from datetime import datetime
from dotenv import load_dotenv
import time
import random

# ===== Load environment variables =====
load_dotenv()

# ===== Config logging =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ===== Environment Variables =====
MONGO_HOST = os.getenv('MONGO_HOST', 'localhost')
MONGO_PORT = int(os.getenv('MONGO_PORT', 27017))
MONGO_USERNAME = os.getenv('MONGO_INITDB_ROOT_USERNAME', 'admin')
MONGO_PASSWORD = os.getenv('MONGO_INITDB_ROOT_PASSWORD', 'mongodb_password')
MONGO_DB = os.getenv('MONGO_DB', 'job_db')
TARGET_URL = os.getenv('TARGET_URL', 'https://itviec.com/it-jobs')
SCRAPE_DELAY = int(os.getenv('SCRAPE_DELAY', 2))


class ITviecScraper:
    """
    Class ƒë·ªÉ c√†o d·ªØ li·ªáu t·ª´ ITviec.com b·∫±ng Selenium
    
    Attributes:
        mongo_uri (str): Connection string ƒë·ªÉ k·∫øt n·ªëi MongoDB
        db_name (str): T√™n database trong MongoDB
        collection_name (str): T√™n collection ƒë·ªÉ l∆∞u d·ªØ li·ªáu
    """
    
    def __init__(self, mongo_uri: str, db_name: str, collection_name: str = 'raw_jobs'):
        """
        Kh·ªüi t·∫°o scraper v·ªõi Selenium
        
        Args:
            mongo_uri: MongoDB connection URI
            db_name: Database name
            collection_name: Collection name (default: 'raw_jobs')
        """
        self.mongo_uri = mongo_uri
        self.db_name = db_name
        self.collection_name = collection_name
        self.client = None
        self.db = None
        self.collection = None
        self.driver = None
        
    def _init_driver(self):
        """Kh·ªüi t·∫°o Chrome WebDriver v·ªõi headless mode"""
        try:
            logger.info("üöó ƒêang kh·ªüi t·∫°o Chrome WebDriver...")
            
            chrome_options = Options()
            # chrome_options.add_argument('--headless')  # T·∫°m t·∫Øt headless ƒë·ªÉ debug (n·∫øu ch·∫°y local)
            chrome_options.add_argument('--headless=new') # Ch·∫ø ƒë·ªô headless m·ªõi c·ªßa Chrome, √≠t b·ªã detect h∆°n
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            # Th√™m c√°c arguments ƒë·ªÉ bypass Cloudflare
            chrome_options.add_argument("--disable-extensions")
            chrome_options.add_argument("--proxy-server='direct://'")
            chrome_options.add_argument("--proxy-bypass-list=*")
            chrome_options.add_argument("--start-maximized")
            chrome_options.add_argument('--allow-running-insecure-content')
            chrome_options.add_argument('--ignore-certificate-errors')
            
            # T·∫Øt automation flags
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # Kh·ªüi t·∫°o driver (kh√¥ng d√πng ChromeDriverManager v√¨ c√≥ bug)
            # Chrome ƒë√£ ƒë∆∞·ª£c c√†i trong container, d√πng default chromedriver
            self.driver = webdriver.Chrome(options=chrome_options)
            
            # Apply selenium-stealth
            stealth(self.driver,
                languages=["en-US", "en"],
                vendor="Google Inc.",
                platform="Win32",
                webgl_vendor="Intel Inc.",
                renderer="Intel Iris OpenGL Engine",
                fix_hairline=True,
            )
            
            logger.info("‚úÖ Chrome WebDriver ƒë√£ kh·ªüi t·∫°o th√†nh c√¥ng (v·ªõi Stealth)")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói kh·ªüi t·∫°o WebDriver: {e}")
            raise
    
    def _close_driver(self):
        """ƒê√≥ng WebDriver"""
        if self.driver:
            try:
                self.driver.quit()
                logger.info("‚úÖ ƒê√£ ƒë√≥ng Chrome WebDriver")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è L·ªói khi ƒë√≥ng driver: {e}")
    
    def connect_mongodb(self):
        """K·∫øt n·ªëi t·ªõi MongoDB"""
        try:
            self.client = MongoClient(self.mongo_uri, serverSelectionTimeoutMS=5000)
            # Test connection
            self.client.admin.command('ping')
            self.db = self.client[self.db_name]
            self.collection = self.db[self.collection_name]
            logger.info(f"‚úÖ K·∫øt n·ªëi MongoDB th√†nh c√¥ng - Database: {self.db_name}")
        except errors.ServerSelectionTimeoutError as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi MongoDB: {e}")
            raise
    
    def disconnect_mongodb(self):
        """Ng·∫Øt k·∫øt n·ªëi MongoDB"""
        if self.client:
            self.client.close()
            logger.info("‚úÖ Ng·∫Øt k·∫øt n·ªëi MongoDB")
    
    def fetch_page(self, url: str, max_retries: int = 3) -> str:
        """
        L·∫•y HTML t·ª´ URL b·∫±ng Selenium
        
        Args:
            url: URL c·ªßa trang web
            max_retries: S·ªë l·∫ßn retry t·ªëi ƒëa
            
        Returns:
            HTML content (str)
        """
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    wait_time = (2 ** attempt) + random.uniform(1, 3)
                    logger.info(f"‚è≥ Retry {attempt}/{max_retries} sau {wait_time:.1f}s...")
                    time.sleep(wait_time)
                
                logger.info(f"üåê ƒêang t·∫£i: {url}")
                self.driver.get(url)
                
                # ƒê·ª£i trang load (ƒë·ª£i job listings xu·∫•t hi·ªán)
                try:
                    WebDriverWait(self.driver, 15).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "div.job-content, div[class*='job'], h3"))
                    )
                    logger.info("‚úÖ Trang ƒë√£ load xong")
                except TimeoutException:
                    logger.warning("‚ö†Ô∏è Timeout ch·ªù elements, nh∆∞ng v·∫´n ti·∫øp t·ª•c...")
                
                # Th√™m delay ng·∫´u nhi√™n ƒë·ªÉ gi·ªëng ng∆∞·ªùi d√πng th·∫≠t
                time.sleep(random.uniform(3, 5))
                
                # L·∫•y page source
                html = self.driver.page_source
                
                # Ki·ªÉm tra xem c√≥ ph·∫£i trang Cloudflare challenge kh√¥ng
                if "Just a moment" in html or "Checking your browser" in html:
                    logger.warning("‚ö†Ô∏è G·∫∑p Cloudflare challenge, ƒë·ª£i...")
                    time.sleep(10)  # ƒê·ª£i Cloudflare solve
                    html = self.driver.page_source
                
                logger.info(f"‚úÖ L·∫•y ƒë∆∞·ª£c {len(html)} bytes HTML")
                return html
                
            except WebDriverException as e:
                logger.error(f"‚ùå L·ªói WebDriver (attempt {attempt + 1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    raise
            except Exception as e:
                logger.error(f"‚ùå L·ªói fetch page (attempt {attempt + 1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    raise
    
    def parse_job_listing(self, job_html) -> dict:
        """
        Parse 1 job listing t·ª´ HTML
        
        Args:
            job_html: BeautifulSoup element ch·ª©a th√¥ng tin 1 job
            
        Returns:
            dict ch·ª©a th√¥ng tin job
        """
        try:
            # Job Title & URL
            # Title n·∫±m trong th·∫ª h3, URL n·∫±m trong attribute data-url c·ªßa h3
            title_elem = job_html.find('h3')
            if title_elem:
                title = title_elem.get_text(strip=True)
                job_url = title_elem.get('data-url')
                if not job_url:
                    # Fallback n·∫øu kh√¥ng c√≥ data-url
                    a_tag = title_elem.find('a')
                    if a_tag:
                        job_url = a_tag.get('href')
            else:
                title = "N/A"
                job_url = "N/A"
            
            # Chu·∫©n h√≥a URL
            if job_url and job_url != "N/A" and not job_url.startswith('http'):
                job_url = f"https://itviec.com{job_url}"

            # Company Name
            # T√¨m t·∫•t c·∫£ th·∫ª a c√≥ href ch·ª©a /companies/
            company_links = job_html.find_all('a', href=lambda x: x and '/companies/' in x)
            company = "N/A"
            for link in company_links:
                text = link.get_text(strip=True)
                if text:
                    company = text
                    break
            
            # Location
            # T√¨m div c√≥ c√°c class n√†y (d√πng CSS selector cho ch√≠nh x√°c)
            location_elem = job_html.select_one('.text-rich-grey.text-truncate.text-nowrap')
            location = location_elem.get_text(strip=True) if location_elem else "N/A"
            
            # Salary
            salary_elem = job_html.find('div', class_='salary')
            salary = salary_elem.get_text(strip=True) if salary_elem else "N/A"
            
            # Skills (Tags)
            skills = []
            tag_list = job_html.find('div', attrs={'data-controller': 'responsive-tag-list'})
            if tag_list:
                skills = [tag.get_text(strip=True) for tag in tag_list.find_all('a')]

            return {
                "title": title,
                "url": job_url,
                "company": company,
                "location": location,
                "salary": salary,
                "skills": skills,
                "scraped_at": datetime.utcnow().isoformat()
            }

        except Exception as e:
            logger.error(f"‚ùå L·ªói parse job: {e}")
            return None

            return None
    
    def scrape_jobs(self, base_url: str, max_pages: int = 5) -> list:
        """
        C√†o danh s√°ch jobs theo t·ª´ng Keyword (Skill) ƒë·ªÉ tr√°nh Pagination c·ªßa Cloudflare.
        Thay v√¨ duy·ªát ?page=1,2,3 (b·ªã block), ta duy·ªát /it-jobs/java, /it-jobs/python...
        
        Args:
            base_url: URL g·ªëc (kh√¥ng d√πng nhi·ªÅu trong strategy n√†y)
            max_pages: S·ªë l∆∞·ª£ng keyword mu·ªën c√†o (t·∫°m d√πng tham s·ªë n√†y)
            
        Returns:
            List c√°c job dicts
        """
        all_jobs = []
        
        # Danh s√°ch keywords ph·ªï bi·∫øn ƒë·ªÉ c√†o
        # keywords = [
        #     "java", "python", "react", "javascript", "net", 
        #     "tester", "php", "android", "ios", "node-js",
        #     "business-analyst", "project-manager", "data-engineer"
        # ]
        
        # TEST MODE: Ch·ªâ c√†o 1 keyword ƒë·ªÉ test nhanh
        keywords = ["java"]
        
        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng keyword n·∫øu c·∫ßn
        target_keywords = keywords[:max_pages] if max_pages > 0 else keywords
        
        # Kh·ªüi t·∫°o driver
        self._init_driver()
        
        try:
            for index, keyword in enumerate(target_keywords):
                # Anti-bot: Reset driver ho√†n to√†n gi·ªØa c√°c keyword ƒë·ªÉ x√≥a s·∫°ch session/fingerprint
                if index > 0:
                    try:
                        logger.info("üîÑ ƒêang kh·ªüi ƒë·ªông l·∫°i WebDriver ƒë·ªÉ tr√°nh b·ªã block...")
                        self._close_driver()
                        
                        delay = random.uniform(20, 40)
                        logger.info(f"üò¥ Ngh·ªâ {delay:.1f}s tr∆∞·ªõc khi t·∫°o session m·ªõi...")
                        time.sleep(delay)
                        
                        self._init_driver()
                    except Exception as e:
                        logger.error(f"‚ùå L·ªói khi restart driver: {e}")
                        # N·∫øu l·ªói restart, c·ªë g·∫Øng init l·∫°i n·∫øu ch∆∞a c√≥
                        if not self.driver:
                            self._init_driver()

                # X√¢y d·ª±ng URL theo keyword
                url = f"https://itviec.com/it-jobs/{keyword}"
                
                logger.info(f"üìÑ [{index+1}/{len(target_keywords)}] ƒêang x·ª≠ l√Ω keyword: {keyword} -> {url}")
                
                try:
                    # L·∫•y HTML c·ªßa trang
                    html = self.fetch_page(url)
                    
                    # Ki·ªÉm tra Cloudflare Challenge trong HTML
                    if "Verify you are human" in html or "Just a moment" in html:
                        # N·∫øu file l·ªõn (>100KB), c√≥ th·ªÉ l√† false positive?
                        if len(html) > 100000:
                             logger.info(f"‚ö†Ô∏è Ph√°t hi·ªán t·ª´ kh√≥a Cloudflare nh∆∞ng HTML l·ªõn ({len(html)} bytes) -> False Positive. Ti·∫øp t·ª•c parse...")
                        else:
                            logger.warning(f"‚ö†Ô∏è Cloudflare Challenge detected for {keyword}. Saving HTML for debug...")
                            try:
                                with open(f"/tmp/debug_{keyword}_challenge.html", "w", encoding="utf-8") as f:
                                    f.write(html)
                            except Exception as e:
                                logger.warning(f"Kh√¥ng th·ªÉ l∆∞u file debug: {e}")
                                
                            logger.warning("Waiting 60s...")
                            time.sleep(60)
                            html = self.fetch_page(url) # Retry 1 l·∫ßn

                    # Ki·ªÉm tra n·∫øu b·ªã block (HTML qu√° ng·∫Øn)
                    if len(html) < 30000:
                        logger.warning(f"‚ö†Ô∏è HTML qu√° ng·∫Øn ({len(html)} bytes), c√≥ th·ªÉ b·ªã block. ƒê·ª£i 30s v√† th·ª≠ l·∫°i...")
                        time.sleep(30)
                        html = self.fetch_page(url) # Retry 1 l·∫ßn
                    
                    # Parse v·ªõi BeautifulSoup
                    soup = BeautifulSoup(html, 'lxml')
                    
                    # T√¨m t·∫•t c·∫£ job listings
                    # Selector c·∫≠p nh·∫≠t: T√¨m div c√≥ class ch·ª©a 'job' v√† b√™n trong c√≥ h3
                    job_cards = []
                    
                    # Strategy 1: T√¨m theo class c·ª• th·ªÉ (th∆∞·ªùng l√† job-card ho·∫∑c job-content)
                    candidates = soup.select('div[class*="job"]')
                    
                    for card in candidates:
                        # Filter: Ph·∫£i c√≥ H3 (Title) v√† kh√¥ng ph·∫£i l√† "Job Alert" hay qu·∫£ng c√°o
                        if card.find('h3') and not card.find('form'): 
                            # Ki·ªÉm tra ƒë·ªô d√†i text ƒë·ªÉ lo·∫°i b·ªè c√°c div r√°c
                            if len(card.get_text()) > 50:
                                job_cards.append(card)
                    
                    # Lo·∫°i b·ªè duplicate cards (do c·∫•u tr√∫c l·ªìng nhau)
                    # Ch·ªâ gi·ªØ l·∫°i card con nh·∫•t ho·∫∑c card cha chu·∫©n
                    # ƒê∆°n gi·∫£n h√≥a: N·∫øu t√¨m th·∫•y qu√° nhi·ªÅu, ch·ªâ l·∫•y 20-30 c√°i ƒë·∫ßu ti√™n (th∆∞·ªùng l√† s·ªë job tr√™n 1 trang)
                    if len(job_cards) > 40:
                        job_cards = job_cards[:40]
                    
                    logger.info(f"üì¶ Keyword '{keyword}': T√¨m th·∫•y {len(job_cards)} th·∫ª jobs ti·ªÅm nƒÉng")
                    
                    # Parse jobs
                    current_page_jobs = []
                    for job_card in job_cards:
                        job_data = self.parse_job_listing(job_card)
                        if job_data:
                            # Validate d·ªØ li·ªáu r√°c
                            if job_data['title'] != "N/A" and job_data['url'] != "N/A":
                                current_page_jobs.append(job_data)
                    
                    # L·ªçc duplicate (so v·ªõi c√°c keyword tr∆∞·ªõc)
                    seen_urls = {job['url'] for job in all_jobs}
                    new_jobs = [job for job in current_page_jobs if job['url'] not in seen_urls]
                    
                    all_jobs.extend(new_jobs)
                    logger.info(f"‚úÖ Keyword '{keyword}': Th√™m {len(new_jobs)} jobs m·ªõi (T·ªïng: {len(all_jobs)})")
                    
                    # Delay gi·ªØa c√°c keyword
                    if index < len(target_keywords) - 1:
                        delay = random.uniform(5, 8)
                        logger.info(f"üò¥ Ngh·ªâ {delay:.1f}s tr∆∞·ªõc keyword ti·∫øp theo...")
                        time.sleep(delay)
                        
                except Exception as e:
                    logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω keyword '{keyword}': {e}")
                    continue
            
            logger.info(f"‚úÖ T·ªïng c·ªông c√†o ƒë∆∞·ª£c {len(all_jobs)} jobs t·ª´ {len(target_keywords)} keywords")
            return all_jobs
            
        finally:
            # ƒê·∫£m b·∫£o ƒë√≥ng driver
            self._close_driver()
    
    def save_to_mongodb(self, jobs: list) -> bool:
        """
        L∆∞u danh s√°ch jobs v√†o MongoDB
        
        Args:
            jobs: List c√°c job dicts
            
        Returns:
            True n·∫øu th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
        """
        if not jobs:
            logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u")
            return False
        
        try:
            # X√≥a d·ªØ li·ªáu c≈© (optional - c√≥ th·ªÉ comment out n·∫øu mu·ªën append)
            # self.collection.delete_many({})
            # logger.info("üóëÔ∏è ƒê√£ x√≥a d·ªØ li·ªáu c≈©")
            
            # Insert m·ªõi
            result = self.collection.insert_many(jobs)
            logger.info(f"‚úÖ ƒê√£ l∆∞u {len(result.inserted_ids)} jobs v√†o MongoDB")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi l∆∞u v√†o MongoDB: {e}")
            return False
    
    def get_statistics(self) -> dict:
        """
        L·∫•y th·ªëng k√™ d·ªØ li·ªáu trong MongoDB
        
        Returns:
            Dict ch·ª©a c√°c th·ªëng k√™
        """
        try:
            total_jobs = self.collection.count_documents({})
            
            # Th·ªëng k√™ theo company
            companies = self.collection.distinct('company')
            
            # Th·ªëng k√™ theo location
            locations = self.collection.distinct('location')
            
            stats = {
                'total_jobs': total_jobs,
                'unique_companies': len(companies),
                'unique_locations': len(locations),
                'last_scraped': datetime.utcnow().isoformat()
            }
            
            logger.info(f"üìä Th·ªëng k√™: {json.dumps(stats, indent=2, ensure_ascii=False)}")
            return stats
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi l·∫•y th·ªëng k√™: {e}")
            return {}


# ===== Main execution (for testing) =====
if __name__ == "__main__":
    # MongoDB URI
    mongo_uri = f"mongodb://{MONGO_USERNAME}:{MONGO_PASSWORD}@{MONGO_HOST}:{MONGO_PORT}/"
    
    # Kh·ªüi t·∫°o scraper
    scraper = ITviecScraper(
        mongo_uri=mongo_uri,
        db_name=MONGO_DB
    )
    
    try:
        # K·∫øt n·ªëi MongoDB
        scraper.connect_mongodb()
        
        # C√†o d·ªØ li·ªáu
        logger.info(f"üöÄ B·∫Øt ƒë·∫ßu c√†o t·ª´: {TARGET_URL}")
        jobs = scraper.scrape_jobs(TARGET_URL, max_pages=3)
        
        # L∆∞u v√†o MongoDB
        success = scraper.save_to_mongodb(jobs)
        
        # L·∫•y th·ªëng k√™
        stats = scraper.get_statistics()
        
        logger.info("üéâ Ho√†n th√†nh!")
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói: {e}")
        
    finally:
        # Ng·∫Øt k·∫øt n·ªëi
        scraper.disconnect_mongodb()
