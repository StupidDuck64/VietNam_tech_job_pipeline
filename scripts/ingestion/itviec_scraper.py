"""
===== ITviec Job Scraper =====
Script c√†o d·ªØ li·ªáu vi·ªác l√†m t·ª´ ITviec.com
S·ª≠ d·ª•ng: Requests + BeautifulSoup
L∆∞u v√†o: MongoDB

Author: Data Engineering Team
Date: December 2025
"""

import requests
from bs4 import BeautifulSoup
from pymongo import MongoClient, errors
import json
import logging
import os
from datetime import datetime
from dotenv import load_dotenv
import time

# ===== Load environment variables =====
load_dotenv()

# ===== Config logging =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ===== Environment Variables =====
MONGO_HOST = os.getenv('MONGO_HOST', 'localhost')
MONGO_PORT = int(os.getenv('MONGO_PORT', 27017))
MONGO_USERNAME = os.getenv('MONGO_INITDB_ROOT_USERNAME', 'admin')
MONGO_PASSWORD = os.getenv('MONGO_INITDB_ROOT_PASSWORD', 'mongodb_password')
MONGO_DB = os.getenv('MONGO_DB', 'job_db')
TARGET_URL = os.getenv('TARGET_URL', 'https://itviec.com/it-jobs/data-engineer')
SCRAPE_DELAY = int(os.getenv('SCRAPE_DELAY', 2))


class ITviecScraper:
    """
    Class ƒë·ªÉ c√†o d·ªØ li·ªáu t·ª´ ITviec.com
    
    Attributes:
        mongo_uri (str): Connection string ƒë·ªÉ k·∫øt n·ªëi MongoDB
        db_name (str): T√™n database trong MongoDB
        collection_name (str): T√™n collection ƒë·ªÉ l∆∞u d·ªØ li·ªáu
    """
    
    def __init__(self, mongo_uri: str, db_name: str, collection_name: str = 'raw_jobs'):
        """
        Kh·ªüi t·∫°o scraper
        
        Args:
            mongo_uri: MongoDB connection URI
            db_name: Database name
            collection_name: Collection name (default: 'raw_jobs')
        """
        self.mongo_uri = mongo_uri
        self.db_name = db_name
        self.collection_name = collection_name
        self.client = None
        self.db = None
        self.collection = None
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,vi;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
        
    def connect_mongodb(self):
        """K·∫øt n·ªëi t·ªõi MongoDB"""
        try:
            self.client = MongoClient(self.mongo_uri, serverSelectionTimeoutMS=5000)
            # Test connection
            self.client.admin.command('ping')
            self.db = self.client[self.db_name]
            self.collection = self.db[self.collection_name]
            logger.info(f"‚úÖ K·∫øt n·ªëi MongoDB th√†nh c√¥ng - Database: {self.db_name}")
        except errors.ServerSelectionTimeoutError as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi MongoDB: {e}")
            raise
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi k·∫øt n·ªëi MongoDB: {e}")
            raise
    
    def disconnect_mongodb(self):
        """Ng·∫Øt k·∫øt n·ªëi MongoDB"""
        if self.client:
            self.client.close()
            logger.info("‚úÖ Ng·∫Øt k·∫øt n·ªëi MongoDB")
    
    def fetch_page(self, url: str, max_retries: int = 3) -> str:
        """
        L·∫•y HTML t·ª´ URL v·ªõi retry logic
        
        Args:
            url: URL c·ªßa trang web
            max_retries: S·ªë l·∫ßn retry t·ªëi ƒëa
            
        Returns:
            HTML content (str)
        """
        for attempt in range(max_retries):
            try:
                # Th√™m delay ng·∫´u nhi√™n ƒë·ªÉ tr√°nh pattern detection
                if attempt > 0:
                    wait_time = (2 ** attempt) + (attempt * 0.5)  # Exponential backoff
                    logger.info(f"‚è≥ Retry {attempt}/{max_retries} sau {wait_time}s...")
                    time.sleep(wait_time)
                
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                logger.info(f"‚úÖ L·∫•y d·ªØ li·ªáu t·ª´: {url}")
                return response.text
                
            except requests.HTTPError as e:
                if e.response.status_code == 403:
                    logger.warning(f"‚ö†Ô∏è 403 Forbidden (attempt {attempt + 1}/{max_retries})")
                    if attempt == max_retries - 1:
                        logger.error(f"‚ùå H·∫øt retry, v·∫´n b·ªã 403: {url}")
                        raise
                else:
                    raise
            except requests.RequestException as e:
                logger.error(f"‚ùå L·ªói fetch page (attempt {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    raise
    
    def parse_job_listing(self, job_html) -> dict:
        """
        Parse 1 job listing t·ª´ HTML
        
        Args:
            job_html: BeautifulSoup element c·ªßa 1 job listing
            
        Returns:
            Dict ch·ª©a th√¥ng tin job
        """
        try:
            job_data = {}
            
            # ===== T√¨m job title =====
            job_title_elem = job_html.find('h2', class_='job__title')
            job_data['job_title'] = job_title_elem.get_text(strip=True) if job_title_elem else 'N/A'
            
            # ===== T√¨m job URL =====
            job_link_elem = job_html.find('a', class_='job__link')
            job_data['job_url'] = job_link_elem['href'] if job_link_elem else 'N/A'
            
            # ===== T√¨m company name =====
            company_elem = job_html.find('span', class_='company-name')
            job_data['company_name'] = company_elem.get_text(strip=True) if company_elem else 'N/A'
            
            # ===== T√¨m location =====
            location_elem = job_html.find('span', class_='location')
            job_data['location'] = location_elem.get_text(strip=True) if location_elem else 'N/A'
            
            # ===== T√¨m salary =====
            salary_elem = job_html.find('span', class_='salary')
            job_data['salary'] = salary_elem.get_text(strip=True) if salary_elem else 'Not disclosed'
            
            # ===== T√¨m job description (short preview) =====
            desc_elem = job_html.find('div', class_='job__description')
            job_data['description_preview'] = desc_elem.get_text(strip=True) if desc_elem else 'N/A'
            
            # ===== Add metadata =====
            job_data['scraped_at'] = datetime.now().isoformat()
            job_data['source'] = 'itviec.com'
            
            return job_data
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è L·ªói parse job listing: {e}")
            return None
    
    def scrape_jobs(self, url: str, max_pages: int = 1) -> list:
        """
        C√†o d·ªØ li·ªáu t·ª´ ITviec.com
        
        Args:
            url: URL trang search
            max_pages: S·ªë trang c·∫ßn c√†o (default: 1)
            
        Returns:
            List of job dictionaries
        """
        all_jobs = []
        
        for page in range(1, max_pages + 1):
            try:
                # ===== T·∫°o URL v·ªõi pagination =====
                page_url = f"{url}?page={page}" if page > 1 else url
                logger.info(f"üìÑ ƒêang c√†o trang {page}: {page_url}")
                
                # ===== Fetch HTML =====
                html = self.fetch_page(page_url)
                soup = BeautifulSoup(html, 'html.parser')
                
                # ===== T√¨m t·∫•t c·∫£ job listings =====
                job_listings = soup.find_all('div', class_='job-item')
                logger.info(f"üîç T√¨m th·∫•y {len(job_listings)} job listings tr√™n trang {page}")
                
                if not job_listings:
                    logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y job listings tr√™n trang {page}")
                    break
                
                # ===== Parse t·ª´ng job =====
                for job_html in job_listings:
                    job_data = self.parse_job_listing(job_html)
                    if job_data:
                        all_jobs.append(job_data)
                
                # ===== Delay ƒë·ªÉ tr√°nh b·ªã ch·∫∑n IP (v·ªõi random jitter) =====
                import random
                delay = SCRAPE_DELAY + random.uniform(0.5, 2.0)
                logger.info(f"‚è≥ Ch·ªù {delay:.1f}s tr∆∞·ªõc khi c√†o trang ti·∫øp...")
                time.sleep(delay)
                
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi c√†o trang {page}: {e}")
                continue
        
        logger.info(f"‚úÖ T·ªïng c·ªông c√†o ƒë∆∞·ª£c {len(all_jobs)} jobs")
        return all_jobs
    
    def save_to_mongodb(self, jobs: list) -> bool:
        """
        L∆∞u d·ªØ li·ªáu v√†o MongoDB
        
        Args:
            jobs: List of job dictionaries
            
        Returns:
            True n·∫øu l∆∞u th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
        """
        try:
            if not jobs:
                logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u")
                return False
            
            # ===== X√≥a d·ªØ li·ªáu c≈© (optional) =====
            # self.collection.delete_many({})
            # logger.info("üóëÔ∏è X√≥a d·ªØ li·ªáu c≈© trong collection")
            
            # ===== Insert d·ªØ li·ªáu m·ªõi =====
            result = self.collection.insert_many(jobs)
            logger.info(f"‚úÖ L∆∞u {len(result.inserted_ids)} jobs v√†o MongoDB")
            
            return True
        
        except errors.DuplicateKeyError:
            logger.warning("‚ö†Ô∏è M·ªôt s·ªë jobs ƒë√£ t·ªìn t·∫°i trong database")
            return True
        except Exception as e:
            logger.error(f"‚ùå L·ªói l∆∞u v√†o MongoDB: {e}")
            return False
    
    def get_statistics(self) -> dict:
        """
        L·∫•y th·ªëng k√™ t·ª´ collection
        
        Returns:
            Dict ch·ª©a s·ªë l∆∞·ª£ng records, etc
        """
        try:
            total_jobs = self.collection.count_documents({})
            
            # ===== Th·ªëng k√™ c√¥ng ty =====
            unique_companies = self.collection.distinct('company_name')
            
            # ===== Th·ªëng k√™ location =====
            unique_locations = self.collection.distinct('location')
            
            stats = {
                'total_jobs': total_jobs,
                'unique_companies': len(unique_companies),
                'unique_locations': len(unique_locations),
                'last_scraped': datetime.now().isoformat()
            }
            
            logger.info(f"üìä Th·ªëng k√™: {json.dumps(stats, indent=2, ensure_ascii=False)}")
            return stats
        
        except Exception as e:
            logger.error(f"‚ùå L·ªói l·∫•y th·ªëng k√™: {e}")
            return {}


def main():
    """Main function"""
    
    # ===== T·∫°o MongoDB URI =====
    mongo_uri = f"mongodb://{MONGO_USERNAME}:{MONGO_PASSWORD}@{MONGO_HOST}:{MONGO_PORT}/"
    
    # ===== Kh·ªüi t·∫°o scraper =====
    scraper = ITviecScraper(
        mongo_uri=mongo_uri,
        db_name=MONGO_DB
    )
    
    try:
        # ===== K·∫øt n·ªëi MongoDB =====
        scraper.connect_mongodb()
        
        # ===== C√†o d·ªØ li·ªáu =====
        logger.info(f"üöÄ B·∫Øt ƒë·∫ßu c√†o t·ª´: {TARGET_URL}")
        jobs = scraper.scrape_jobs(TARGET_URL, max_pages=2)
        
        # ===== L∆∞u v√†o MongoDB =====
        success = scraper.save_to_mongodb(jobs)
        
        if success:
            # ===== Hi·ªÉn th·ªã th·ªëng k√™ =====
            scraper.get_statistics()
            logger.info("‚ú® Ho√†n th√†nh scraping!")
        else:
            logger.error("‚ùå Scraping th·∫•t b·∫°i!")
            
    except Exception as e:
        logger.error(f"‚ùå L·ªói chung: {e}")
    finally:
        # ===== Ng·∫Øt k·∫øt n·ªëi =====
        scraper.disconnect_mongodb()


if __name__ == '__main__':
    main()
