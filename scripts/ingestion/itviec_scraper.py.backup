"""
===== ITviec Job Scraper =====
Script c√†o d·ªØ li·ªáu vi·ªác l√†m t·ª´ ITviec.com
S·ª≠ d·ª•ng: Requests + BeautifulSoup
L∆∞u v√†o: MongoDB

Author: Data Engineering Team
Date: December 2025
"""

import cloudscraper
from bs4 import BeautifulSoup
from pymongo import MongoClient, errors
import json
import logging
import os
from datetime import datetime
from dotenv import load_dotenv
import time
import random
from urllib.parse import unquote

# ===== Load environment variables =====
load_dotenv()

# ===== Config logging =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ===== Environment Variables =====
MONGO_HOST = os.getenv('MONGO_HOST', 'localhost')
MONGO_PORT = int(os.getenv('MONGO_PORT', 27017))
MONGO_USERNAME = os.getenv('MONGO_INITDB_ROOT_USERNAME', 'admin')
MONGO_PASSWORD = os.getenv('MONGO_INITDB_ROOT_PASSWORD', 'mongodb_password')
MONGO_DB = os.getenv('MONGO_DB', 'job_db')
TARGET_URL = os.getenv('TARGET_URL', 'https://itviec.com/it-jobs/data-engineer')
SCRAPE_DELAY = int(os.getenv('SCRAPE_DELAY', 2))
ITVIEC_SESSION_COOKIE = os.getenv('ITVIEC_SESSION_COOKIE', '')
# Cloudflare cookies
CFUVID_COOKIE = os.getenv('CFUVID_COOKIE', '')
GA_COOKIE = os.getenv('GA_COOKIE', '')
CFZ_ADOBE_COOKIE = os.getenv('CFZ_ADOBE_COOKIE', '')
CFZ_GOOGLE_COOKIE = os.getenv('CFZ_GOOGLE_COOKIE', '')
KNDCTR_COOKIE = os.getenv('KNDCTR_COOKIE', '')


class ITviecScraper:
    """
    Class ƒë·ªÉ c√†o d·ªØ li·ªáu t·ª´ ITviec.com
    
    Attributes:
        mongo_uri (str): Connection string ƒë·ªÉ k·∫øt n·ªëi MongoDB
        db_name (str): T√™n database trong MongoDB
        collection_name (str): T√™n collection ƒë·ªÉ l∆∞u d·ªØ li·ªáu
    """
    
    def __init__(self, mongo_uri: str, db_name: str, collection_name: str = 'raw_jobs'):
        """
        Kh·ªüi t·∫°o scraper
        
        Args:
            mongo_uri: MongoDB connection URI
            db_name: Database name
            collection_name: Collection name (default: 'raw_jobs')
        """
        self.mongo_uri = mongo_uri
        self.db_name = db_name
        self.collection_name = collection_name
        self.client = None
        self.db = None
        self.collection = None
        # S·ª≠ d·ª•ng cloudscraper ƒë·ªÉ bypass Cloudflare
        self.scraper = cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'desktop': True
            },
            delay=10  # Th√™m delay ƒë·ªÉ tr√°nh rate limiting
        )
        # Th√™m headers ƒë·ªÉ gi·∫£ l·∫≠p browser th·∫≠t
        self.scraper.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,vi;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'max-age=0',
            'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://itviec.com/',
        })
        # N·∫øu c√≥ manual cookies, s·ª≠ d·ª•ng lu√¥n
        if ITVIEC_SESSION_COOKIE:
            logger.info("üîë S·ª≠ d·ª•ng manual session cookies (ITViec + Cloudflare)")
            # URL decode cookie value tr∆∞·ªõc khi set (v√¨ cookie trong .env ƒë√£ b·ªã encode)
            decoded_session = unquote(ITVIEC_SESSION_COOKIE)
            logger.info(f"üîì Decoded session cookie length: {len(decoded_session)} chars")
            # Set cookies v·ªõi path='/' ƒë·ªÉ ƒë·∫£m b·∫£o cookies ƒë∆∞·ª£c g·ª≠i cho m·ªçi request
            self.scraper.cookies.set('_ITViec_session', decoded_session, domain='itviec.com', path='/')
            # Set Cloudflare cookies
            if CFUVID_COOKIE:
                self.scraper.cookies.set('__cfuvid', CFUVID_COOKIE, domain='.itviec.com', path='/')
            if GA_COOKIE:
                self.scraper.cookies.set('_ga', GA_COOKIE, domain='.itviec.com', path='/')
            if CFZ_ADOBE_COOKIE:
                self.scraper.cookies.set('_cfz_adobe', CFZ_ADOBE_COOKIE, domain='.itviec.com', path='/')
            if CFZ_GOOGLE_COOKIE:
                self.scraper.cookies.set('_cfz_google-analytics_v4', CFZ_GOOGLE_COOKIE, domain='.itviec.com', path='/')
            if KNDCTR_COOKIE:
                self.scraper.cookies.set('kndctr_8AD56F28618A50850A495FB6_AdobeOrg_identity', KNDCTR_COOKIE, domain='.itviec.com', path='/')
            
            # Log cookies ƒë·ªÉ debug
            cookie_count = len(self.scraper.cookies)
            cookie_names = [c.name for c in self.scraper.cookies]
            logger.info(f"‚úÖ ƒê√£ set {cookie_count} cookies: {', '.join(cookie_names)}")
        else:
            # Kh·ªüi t·∫°o session b·∫±ng c√°ch visit homepage tr∆∞·ªõc
            self._init_session()
    
    def _init_session(self):
        """Kh·ªüi t·∫°o session b·∫±ng c√°ch visit homepage ƒë·ªÉ l·∫•y cookies"""
        try:
            logger.info("üîê ƒêang kh·ªüi t·∫°o session v·ªõi ITviec...")
            response = self.scraper.get('https://itviec.com/', timeout=15)
            if response.status_code == 200:
                logger.info(f"‚úÖ Session initialized. Cookies: {len(self.scraper.cookies)} items")
            else:
                logger.warning(f"‚ö†Ô∏è Session init status: {response.status_code}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ init session: {e}")
        
    def connect_mongodb(self):
        """K·∫øt n·ªëi t·ªõi MongoDB"""
        try:
            self.client = MongoClient(self.mongo_uri, serverSelectionTimeoutMS=5000)
            # Test connection
            self.client.admin.command('ping')
            self.db = self.client[self.db_name]
            self.collection = self.db[self.collection_name]
            logger.info(f"‚úÖ K·∫øt n·ªëi MongoDB th√†nh c√¥ng - Database: {self.db_name}")
        except errors.ServerSelectionTimeoutError as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi MongoDB: {e}")
            raise
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi k·∫øt n·ªëi MongoDB: {e}")
            raise
    
    def disconnect_mongodb(self):
        """Ng·∫Øt k·∫øt n·ªëi MongoDB"""
        if self.client:
            self.client.close()
            logger.info("‚úÖ Ng·∫Øt k·∫øt n·ªëi MongoDB")
    
    def fetch_page(self, url: str, max_retries: int = 3) -> str:
        """
        L·∫•y HTML t·ª´ URL v·ªõi cloudscraper (bypass Cloudflare)
        
        Args:
            url: URL c·ªßa trang web
            max_retries: S·ªë l·∫ßn retry t·ªëi ƒëa
            
        Returns:
            HTML content (str)
        """
        for attempt in range(max_retries):
            try:
                # Th√™m delay ng·∫´u nhi√™n ƒë·ªÉ tr√°nh pattern detection
                if attempt > 0:
                    wait_time = (2 ** attempt) + random.uniform(0.5, 2.0)
                    logger.info(f"‚è≥ Retry {attempt}/{max_retries} sau {wait_time:.1f}s...")
                    time.sleep(wait_time)
                    # Re-init session n·∫øu retry
                    self._init_session()
                
                # Log cookies tr∆∞·ªõc khi request ƒë·ªÉ debug
                request_cookies = list(self.scraper.cookies)
                logger.info(f"üç™ Sending {len(request_cookies)} cookies: {[c.name for c in request_cookies]}")
                
                # S·ª≠ d·ª•ng cloudscraper v·ªõi cookies session
                response = self.scraper.get(url, timeout=30, allow_redirects=True)
                
                # Log status v√† response details ƒë·ªÉ debug
                logger.info(f"üì° Status: {response.status_code}, Response cookies: {len(response.cookies)}")
                logger.info(f"üìÑ Content-Type: {response.headers.get('Content-Type', 'N/A')}")
                logger.info(f"üìè Content length: {len(response.content)} bytes")
                
                # N·∫øu 403, log th√™m th√¥ng tin ƒë·ªÉ debug
                if response.status_code == 403:
                    logger.warning(f"‚ö†Ô∏è 403 Response headers: {dict(response.headers)}")
                    logger.warning(f"‚ö†Ô∏è First 500 chars of response: {response.text[:500]}")
                
                response.raise_for_status()
                logger.info(f"‚úÖ L·∫•y d·ªØ li·ªáu t·ª´: {url}")
                return response.text
                
            except Exception as e:
                logger.error(f"‚ùå L·ªói fetch page (attempt {attempt + 1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    raise
    
    def parse_job_listing(self, job_html) -> dict:
        """
        Parse 1 job listing t·ª´ HTML
        
        Args:
            job_html: BeautifulSoup element c·ªßa 1 job listing
            
        Returns:
            Dict ch·ª©a th√¥ng tin job
        """
        try:
            job_data = {}
            
            # ===== T√¨m job title =====
            job_title_elem = job_html.find('h2', class_='job__title')
            job_data['job_title'] = job_title_elem.get_text(strip=True) if job_title_elem else 'N/A'
            
            # ===== T√¨m job URL =====
            job_link_elem = job_html.find('a', class_='job__link')
            job_data['job_url'] = job_link_elem['href'] if job_link_elem else 'N/A'
            
            # ===== T√¨m company name =====
            company_elem = job_html.find('span', class_='company-name')
            job_data['company_name'] = company_elem.get_text(strip=True) if company_elem else 'N/A'
            
            # ===== T√¨m location =====
            location_elem = job_html.find('span', class_='location')
            job_data['location'] = location_elem.get_text(strip=True) if location_elem else 'N/A'
            
            # ===== T√¨m salary =====
            salary_elem = job_html.find('span', class_='salary')
            job_data['salary'] = salary_elem.get_text(strip=True) if salary_elem else 'Not disclosed'
            
            # ===== T√¨m job description (short preview) =====
            desc_elem = job_html.find('div', class_='job__description')
            job_data['description_preview'] = desc_elem.get_text(strip=True) if desc_elem else 'N/A'
            
            # ===== Add metadata =====
            job_data['scraped_at'] = datetime.now().isoformat()
            job_data['source'] = 'itviec.com'
            
            return job_data
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è L·ªói parse job listing: {e}")
            return None
    
    def scrape_jobs(self, url: str, max_pages: int = 1) -> list:
        """
        C√†o d·ªØ li·ªáu t·ª´ ITviec.com
        
        Args:
            url: URL trang search
            max_pages: S·ªë trang c·∫ßn c√†o (default: 1)
            
        Returns:
            List of job dictionaries
        """
        all_jobs = []
        
        for page in range(1, max_pages + 1):
            try:
                # ===== T·∫°o URL v·ªõi pagination =====
                page_url = f"{url}?page={page}" if page > 1 else url
                logger.info(f"üìÑ ƒêang c√†o trang {page}: {page_url}")
                
                # ===== Fetch HTML =====
                html = self.fetch_page(page_url)
                soup = BeautifulSoup(html, 'html.parser')
                
                # ===== T√¨m t·∫•t c·∫£ job listings =====
                job_listings = soup.find_all('div', class_='job-item')
                logger.info(f"üîç T√¨m th·∫•y {len(job_listings)} job listings tr√™n trang {page}")
                
                if not job_listings:
                    logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y job listings tr√™n trang {page}")
                    break
                
                # ===== Parse t·ª´ng job =====
                for job_html in job_listings:
                    job_data = self.parse_job_listing(job_html)
                    if job_data:
                        all_jobs.append(job_data)
                
                # ===== Delay ƒë·ªÉ tr√°nh b·ªã ch·∫∑n IP (3-6s theo best practice) =====
                delay = random.uniform(3, 6)
                logger.info(f"‚è≥ Ch·ªù {delay:.1f}s tr∆∞·ªõc khi c√†o trang ti·∫øp (gi·∫£ l·∫≠p ng∆∞·ªùi d√πng)...")
                time.sleep(delay)
                
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi c√†o trang {page}: {e}")
                continue
        
        logger.info(f"‚úÖ T·ªïng c·ªông c√†o ƒë∆∞·ª£c {len(all_jobs)} jobs")
        return all_jobs
    
    def save_to_mongodb(self, jobs: list) -> bool:
        """
        L∆∞u d·ªØ li·ªáu v√†o MongoDB
        
        Args:
            jobs: List of job dictionaries
            
        Returns:
            True n·∫øu l∆∞u th√†nh c√¥ng, False n·∫øu th·∫•t b·∫°i
        """
        try:
            if not jobs:
                logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u")
                return False
            
            # ===== X√≥a d·ªØ li·ªáu c≈© (optional) =====
            # self.collection.delete_many({})
            # logger.info("üóëÔ∏è X√≥a d·ªØ li·ªáu c≈© trong collection")
            
            # ===== Insert d·ªØ li·ªáu m·ªõi =====
            result = self.collection.insert_many(jobs)
            logger.info(f"‚úÖ L∆∞u {len(result.inserted_ids)} jobs v√†o MongoDB")
            
            return True
        
        except errors.DuplicateKeyError:
            logger.warning("‚ö†Ô∏è M·ªôt s·ªë jobs ƒë√£ t·ªìn t·∫°i trong database")
            return True
        except Exception as e:
            logger.error(f"‚ùå L·ªói l∆∞u v√†o MongoDB: {e}")
            return False
    
    def get_statistics(self) -> dict:
        """
        L·∫•y th·ªëng k√™ t·ª´ collection
        
        Returns:
            Dict ch·ª©a s·ªë l∆∞·ª£ng records, etc
        """
        try:
            total_jobs = self.collection.count_documents({})
            
            # ===== Th·ªëng k√™ c√¥ng ty =====
            unique_companies = self.collection.distinct('company_name')
            
            # ===== Th·ªëng k√™ location =====
            unique_locations = self.collection.distinct('location')
            
            stats = {
                'total_jobs': total_jobs,
                'unique_companies': len(unique_companies),
                'unique_locations': len(unique_locations),
                'last_scraped': datetime.now().isoformat()
            }
            
            logger.info(f"üìä Th·ªëng k√™: {json.dumps(stats, indent=2, ensure_ascii=False)}")
            return stats
        
        except Exception as e:
            logger.error(f"‚ùå L·ªói l·∫•y th·ªëng k√™: {e}")
            return {}


def main():
    """Main function"""
    
    # ===== T·∫°o MongoDB URI =====
    mongo_uri = f"mongodb://{MONGO_USERNAME}:{MONGO_PASSWORD}@{MONGO_HOST}:{MONGO_PORT}/"
    
    # ===== Kh·ªüi t·∫°o scraper =====
    scraper = ITviecScraper(
        mongo_uri=mongo_uri,
        db_name=MONGO_DB
    )
    
    try:
        # ===== K·∫øt n·ªëi MongoDB =====
        scraper.connect_mongodb()
        
        # ===== C√†o d·ªØ li·ªáu =====
        logger.info(f"üöÄ B·∫Øt ƒë·∫ßu c√†o t·ª´: {TARGET_URL}")
        jobs = scraper.scrape_jobs(TARGET_URL, max_pages=2)
        
        # ===== L∆∞u v√†o MongoDB =====
        success = scraper.save_to_mongodb(jobs)
        
        if success:
            # ===== Hi·ªÉn th·ªã th·ªëng k√™ =====
            scraper.get_statistics()
            logger.info("‚ú® Ho√†n th√†nh scraping!")
        else:
            logger.error("‚ùå Scraping th·∫•t b·∫°i!")
            
    except Exception as e:
        logger.error(f"‚ùå L·ªói chung: {e}")
    finally:
        # ===== Ng·∫Øt k·∫øt n·ªëi =====
        scraper.disconnect_mongodb()


if __name__ == '__main__':
    main()
