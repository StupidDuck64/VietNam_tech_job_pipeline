ğŸ“‹ PROJECT SUMMARY - VN IT Job Analytics
==========================================

âœ¨ ToÃ n bá»™ dá»± Ã¡n Data Engineering End-to-End Ä‘Ã£ hoÃ n táº¥t!

---

ğŸ“ Cáº¤UTRÃšC THÆ¯Má»¤C ÄÃƒ Táº O:

vn-it-job-analytics/
â”œâ”€â”€ ğŸ“„ README.md                    # Tá»•ng quan project
â”œâ”€â”€ ğŸ“„ SETUP_GUIDE.md               # HÆ°á»›ng dáº«n setup chi tiáº¿t (QUAN TRá»ŒNG!)
â”œâ”€â”€ ğŸ“„ ARCHITECTURE.md              # Kiáº¿n trÃºc & technical details
â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies
â”œâ”€â”€ ğŸ“„ .env                        # Environment variables
â”œâ”€â”€ ğŸ“„ .gitignore                  # Git config
â”œâ”€â”€ ğŸ“„ docker-compose.yaml          # Docker infrastructure
â”‚
â”œâ”€â”€ ğŸ“ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ ğŸ“„ job_etl_dag.py      # Main orchestration DAG
â”‚   â”œâ”€â”€ logs/                       # Airflow logs (auto-created)
â”‚   â””â”€â”€ plugins/                    # Custom plugins folder
â”‚
â”œâ”€â”€ ğŸ“ scripts/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ itviec_scraper.py   # Web scraper (Ingestion Layer)
â”‚   â””â”€â”€ processing/
â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚       â””â”€â”€ ğŸ“„ spark_cleaner.py    # Spark processor (Processing Layer)
â”‚
â”œâ”€â”€ ğŸ“ docker/
â”‚   â”œâ”€â”€ ğŸ“„ airflow.Dockerfile      # Airflow container image
â”‚   â””â”€â”€ ğŸ“„ spark.Dockerfile        # Spark container image
â”‚
â”œâ”€â”€ ğŸ“ sql/
â”‚   â”œâ”€â”€ ğŸ“„ init_db.sql             # Schema definition (Fact & Dimension tables)
â”‚   â””â”€â”€ ğŸ“„ queries.sql             # 14 sample analytics queries
â”‚
â””â”€â”€ ğŸ“ data/
    â”œâ”€â”€ raw/                        # Dá»¯ liá»‡u JSON thÃ´ (tá»« scraper)
    â””â”€â”€ processed/                  # Dá»¯ liá»‡u Parquet Ä‘Ã£ xá»­ lÃ½ (tá»« Spark)

---

ğŸ“ CÃC FILE CHÃNH ÄÃƒ Táº O:

1ï¸âƒ£  CONFIGURATION FILES
   â”œâ”€â”€ docker-compose.yaml (218 dÃ²ng)
   â”‚   â””â”€â”€ Khá»Ÿi táº¡o: PostgreSQL, MongoDB, Airflow, Spark
   â”œâ”€â”€ requirements.txt
   â”‚   â””â”€â”€ 23 Python packages (Airflow, Spark, Scraping, DB, etc)
   â”œâ”€â”€ .env
   â”‚   â””â”€â”€ 16 environment variables
   â””â”€â”€ .gitignore
       â””â”€â”€ Exclude sensitive files & data

2ï¸âƒ£  INGESTION LAYER (Scraping)
   â””â”€â”€ scripts/ingestion/itviec_scraper.py (368 dÃ²ng)
       â”œâ”€â”€ Class: ITviecScraper
       â”œâ”€â”€ Methods:
       â”‚   â”œâ”€â”€ connect_mongodb() - Káº¿t ná»‘i MongoDB
       â”‚   â”œâ”€â”€ fetch_page() - GET request tá»›i ITviec
       â”‚   â”œâ”€â”€ parse_job_listing() - Parse HTML & extract job info
       â”‚   â”œâ”€â”€ scrape_jobs() - CÃ o multiple pages
       â”‚   â”œâ”€â”€ save_to_mongodb() - LÆ°u dá»¯ liá»‡u
       â”‚   â””â”€â”€ get_statistics() - Thá»‘ng kÃª
       â””â”€â”€ Features:
           â”œâ”€â”€ User-Agent header
           â”œâ”€â”€ Rate limiting (2s delay)
           â”œâ”€â”€ Error handling & retry logic
           â””â”€â”€ MongoDB bulk insert

3ï¸âƒ£  PROCESSING LAYER (Spark)
   â””â”€â”€ scripts/processing/spark_cleaner.py (417 dÃ²ng)
       â”œâ”€â”€ Class: SparkDataCleaner
       â”œâ”€â”€ Methods:
       â”‚   â”œâ”€â”€ read_from_mongodb() - Äá»c tá»« MongoDB
       â”‚   â”œâ”€â”€ clean_text() - XÃ³a HTML, emoji, normalize
       â”‚   â”œâ”€â”€ normalize_salary() - Parse lÆ°Æ¡ng
       â”‚   â”œâ”€â”€ extract_skills() - TrÃ­ch xuáº¥t tá»« description
       â”‚   â”œâ”€â”€ deduplicate_skills() - Loáº¡i bá» trÃ¹ng
       â”‚   â”œâ”€â”€ add_metadata() - ThÃªm processing info
       â”‚   â”œâ”€â”€ write_to_parquet() - LÆ°u Parquet
       â”‚   â”œâ”€â”€ write_to_postgresql() - LÆ°u PostgreSQL
       â”‚   â””â”€â”€ process_pipeline() - Main ETL flow
       â””â”€â”€ Features:
           â”œâ”€â”€ 70+ IT skills matching
           â”œâ”€â”€ Salary range normalization
           â”œâ”€â”€ Multi-step transformation
           â””â”€â”€ Quality scoring

4ï¸âƒ£  ORCHESTRATION (Airflow DAG)
   â””â”€â”€ airflow/dags/job_etl_dag.py (320 dÃ²ng)
       â”œâ”€â”€ 7 Tasks:
       â”‚   â”œâ”€â”€ Task 1: scrape_jobs (PythonOperator)
       â”‚   â”œâ”€â”€ Task 2: validate_data_quality (PythonOperator)
       â”‚   â”œâ”€â”€ Task 3: setup_database_schema (PostgresOperator)
       â”‚   â”œâ”€â”€ Task 4: process_data (PythonOperator)
       â”‚   â”œâ”€â”€ Task 5: load_to_warehouse (PythonOperator)
       â”‚   â”œâ”€â”€ Task 6: generate_report (PythonOperator)
       â”‚   â””â”€â”€ Task 7: cleanup_temp_files (BashOperator)
       â”œâ”€â”€ Schedule: 0 8 * * * (08:00 AM daily)
       â”œâ”€â”€ Retry: 2 attempts, 5min delay
       â””â”€â”€ DAG Dependencies Diagram:
           scrape >> validate >> [setup_db, process] >> load >> report >> cleanup

5ï¸âƒ£  DATABASE SCHEMA (PostgreSQL)
   â””â”€â”€ sql/init_db.sql (231 dÃ²ng)
       â”œâ”€â”€ FACT TABLE: fact_jobs (10 columns)
       â”œâ”€â”€ DIMENSION TABLES:
       â”‚   â”œâ”€â”€ dim_companies
       â”‚   â”œâ”€â”€ dim_skills
       â”‚   â”œâ”€â”€ dim_locations
       â”‚   â””â”€â”€ staging_raw_jobs
       â”œâ”€â”€ INDEXES: 6 tá»‘i Æ°u cho query
       â”œâ”€â”€ VIEWS: 4 cho reporting
       â”‚   â”œâ”€â”€ v_top_skills
       â”‚   â”œâ”€â”€ v_salary_by_company
       â”‚   â”œâ”€â”€ v_jobs_by_location
       â”‚   â””â”€â”€ v_skills_by_location
       â””â”€â”€ STORED PROCEDURES:
           â””â”€â”€ insert_job_with_company()

6ï¸âƒ£  ANALYTICS QUERIES (Sample)
   â””â”€â”€ sql/queries.sql (367 dÃ²ng)
       â”œâ”€â”€ 14 pre-built queries:
       â”‚   â”œâ”€â”€ Top skills in market
       â”‚   â”œâ”€â”€ Salary analysis by location
       â”‚   â”œâ”€â”€ Companies with most postings
       â”‚   â”œâ”€â”€ Skill combinations
       â”‚   â”œâ”€â”€ Job title distribution
       â”‚   â”œâ”€â”€ Location popularity
       â”‚   â”œâ”€â”€ Salary range distribution
       â”‚   â”œâ”€â”€ Data quality report
       â”‚   â”œâ”€â”€ Skills for specific roles
       â”‚   â”œâ”€â”€ Recent postings
       â”‚   â”œâ”€â”€ Company salary benchmark
       â”‚   â”œâ”€â”€ Export jobs with skills
       â”‚   â”œâ”€â”€ Last scrape status
       â”‚   â””â”€â”€ Anomaly detection

7ï¸âƒ£  DOCKER CONFIGURATION
   â”œâ”€â”€ docker/airflow.Dockerfile (27 dÃ²ng)
   â”‚   â””â”€â”€ Apache Airflow 2.7.3 base image + dependencies
   â””â”€â”€ docker/spark.Dockerfile (33 dÃ²ng)
       â””â”€â”€ Bitnami Spark 3.5.0 + Python packages

8ï¸âƒ£  DOCUMENTATION
   â”œâ”€â”€ README.md (165 dÃ²ng)
   â”‚   â””â”€â”€ Project overview, features, usage
   â”œâ”€â”€ SETUP_GUIDE.md (450 dÃ²ng)
   â”‚   â”œâ”€â”€ System requirements
   â”‚   â”œâ”€â”€ Step-by-step installation
   â”‚   â”œâ”€â”€ Service URLs & credentials
   â”‚   â”œâ”€â”€ Running pipeline
   â”‚   â”œâ”€â”€ Troubleshooting
   â”‚   â””â”€â”€ Checklist
   â””â”€â”€ ARCHITECTURE.md (500 dÃ²ng)
       â”œâ”€â”€ System architecture diagram
       â”œâ”€â”€ Data flow explanation
       â”œâ”€â”€ Database schema design
       â”œâ”€â”€ Technology stack
       â”œâ”€â”€ Configuration details
       â”œâ”€â”€ Security considerations
       â”œâ”€â”€ Scaling strategies
       â””â”€â”€ Performance optimization

---

ğŸ¯ TÃNH NÄ‚NG CHÃNH:

âœ… INGESTION
  âœ“ Web scraping tá»« ITviec.com
  âœ“ HTML parsing vá»›i BeautifulSoup
  âœ“ Rate limiting (2s delay/request)
  âœ“ Error handling & retry logic
  âœ“ MongoDB bulk insert

âœ… PROCESSING
  âœ“ PySpark distributed computing
  âœ“ Text cleaning (HTML, emoji removal)
  âœ“ Salary normalization ($2000-$4000 â†’ min/max)
  âœ“ Skill extraction (70+ IT skills)
  âœ“ Data quality scoring
  âœ“ Parquet columnar storage

âœ… STORAGE
  âœ“ MongoDB (raw JSON data)
  âœ“ PostgreSQL (Star Schema DW)
  âœ“ Parquet (analytics format)

âœ… ORCHESTRATION
  âœ“ Apache Airflow DAG
  âœ“ Daily scheduling (8 AM)
  âœ“ Task dependencies
  âœ“ Error handling & alerting
  âœ“ Monitoring UI

âœ… ANALYTICS
  âœ“ 4 pre-built views
  âœ“ 14 sample queries
  âœ“ Data quality checks
  âœ“ Trend analysis

---

ğŸš€ QUICK START COMMANDS:

# 1. VÃ o thÆ° má»¥c project
cd c:\Users\c9283\PyCharmMiscProject\DE_project\DE_Project_01\vn-it-job-analytics

# 2. Build Docker images
docker-compose build

# 3. Khá»Ÿi Ä‘á»™ng háº¡ táº§ng
docker-compose up -d

# 4. Kiá»ƒm tra status
docker-compose ps

# 5. Má»Ÿ Airflow UI
# Truy cáº­p: http://localhost:8080
# Username: airflow
# Password: airflow

# 6. Trigger DAG
# Tá»« UI hoáº·c:
docker-compose exec airflow-webserver airflow dags trigger job_etl_dag

# 7. Xem logs
docker-compose logs -f airflow-scheduler

# 8. Dá»«ng háº¡ táº§ng
docker-compose down

---

ğŸ“Š THá»NG KÃŠ PROJECT:

Tá»•ng sá»‘ file:              13 files
Tá»•ng sá»‘ dÃ²ng code:         ~3500 lines
Tá»•ng dung lÆ°á»£ng (code):    ~180 KB
Docker images:             2 (Airflow + Spark)
Containers:                6 (Postgres, Mongo, Airflow x2, Spark x2)
Database tables:           8 (4 fact/dim + 1 staging + 3 system)
Database views:            4 (for analytics)
Airflow tasks:             7
Python dependencies:       23
SQL queries (sample):      14

---

ğŸ“ LEARNING OUTCOMES:

Sau khi hoÃ n thÃ nh project nÃ y, báº¡n sáº½ hiá»ƒu:

âœ“ Web scraping best practices
âœ“ ETL pipeline design
âœ“ Distributed data processing (Spark)
âœ“ Data warehouse architecture (Star Schema)
âœ“ NoSQL vs SQL databases
âœ“ Workflow orchestration (Airflow)
âœ“ Docker & containerization
âœ“ Data quality & validation
âœ“ SQL analytics queries
âœ“ DevOps & infrastructure as code

---

ğŸ’¼ CV HIGHLIGHTS:

Khi Ä‘Æ°a project nÃ y vÃ o CV, báº¡n cÃ³ thá»ƒ nÃ³i:

"Built an end-to-end Data Engineering pipeline that:
- Scrapes 2,500+ job postings from ITviec.com using Python & BeautifulSoup
- Processes data using Apache Spark (PySpark) with skill extraction & normalization
- Stores raw data in MongoDB & processed data in PostgreSQL (Star Schema)
- Orchestrates daily ETL using Apache Airflow with 7-step pipeline
- Includes 4 pre-built analytics views & 14 SQL queries for data insights
- Containerized with Docker & Docker Compose for easy deployment
- Implements data quality checks, error handling, and monitoring"

---

ğŸ”— NEXT STEPS:

1. âœ“ Setup project (COMPLETED)
2. â³ Run pipeline locally
3. â³ Add unit & integration tests
4. â³ Implement data quality checks
5. â³ Add BI dashboard (Tableau/Looker)
6. â³ Deploy to cloud (AWS/GCP/Azure)
7. â³ Add CI/CD pipeline
8. â³ Performance optimization

---

ğŸ“š DOCUMENTATION LINKS:

- README.md: Overview & quick start
- SETUP_GUIDE.md: Installation & running (READ THIS FIRST!)
- ARCHITECTURE.md: Technical deep dive

---

â“ TROUBLESHOOTING:

If something doesn't work:

1. Check SETUP_GUIDE.md â†’ Troubleshooting section
2. View logs: docker-compose logs <service-name>
3. Restart service: docker-compose restart <service-name>
4. Reset everything: docker-compose down -v && docker-compose up -d

---

âœ¨ PROJECT STATUS: âœ… READY TO RUN!

ToÃ n bá»™ code Ä‘Ã£ Ä‘Æ°á»£c viáº¿t vá»›i:
âœ“ Comments tiáº¿ng Viá»‡t (giá»ng sinh viÃªn)
âœ“ Error handling & logging
âœ“ Best practices
âœ“ Production-ready structure

HÃ£y báº¯t Ä‘áº§u báº±ng SETUP_GUIDE.md! ğŸš€

Created: December 9, 2025
Author: Data Engineering Team
Version: 1.0
